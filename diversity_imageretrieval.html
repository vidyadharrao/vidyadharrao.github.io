<html>
<title>Diverse Yet Efficient Retrieval Using Hash Functions</title>
<head>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);">
</script>

<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<style>
a {
text-decoration: none;
}
h1 {text-align:center}
    div { border: 3px solid; padding: 5px; }  
    div.left { text-align: left; background: #ffcccc; }  
    div.center { text-align: center; background: #ccffcc; }  
    div.right { text-align: right; background: #ccccff; }  
    div.justify { text-align: justify; background: #ffffcc;  font-family:Times New Roman;}  

.dotted-u {border-bottom: 1px dotted red;}
.dashed-u {border-bottom: 1px dashed blue;}
.double-u {border-bottom: 3px double #aa0;}
.groove-u {border-bottom: 5px groove #aa00aa;}
.ridge-u {border-bottom: 5px ridge #0bb;}
.solid-u {border-bottom: 5px solid green; }

table
{
    border: 1px solid #000000;
    border-collapse: collapse;
    border-spacing: 0px;
}
table td
{
    padding: 4px 4px;
}
</style>
</head>

<body>
<div class="justify">
<h1 align="center">Diversity in Image Retrieval </h1>
<!--Summary of the present status of this work is here <a href="EfficientRetrieval_Version1.0.pdf">[pdf]</a> (Last modification: 08/11/13 )!-->
<p align="center">---------------------------- <b> Update: 10/04/2015</b>  ----------------------------</p>
Summary of the results: 
<ul>
<li> Baseline with QIP are computationally expensive: [Typically takes 1hr 40min per query]
<li> Baseline with QIP are memory intensive: [For most queries (17 out of 21) the solver runs for only 3-4 iterations and reports OUT OF MEMORY Err.]
<li> Baseline with QIP solution is same as NN solution: [The optima at real point has almost equal weights to all the images]
<li> LSH with QIP solution show reasonable results in terms of the optima at real point: [However, LSH solution is better in all cases, see at ANN rows in the Tables 16.1, 16.2, 16.3] 
</ul>

<table align="center" border="1">
<caption><b>Table 16.1</b>: Performance of baseline(left), svd(right) methods @ Top 10 </caption>
<tr><td>
<table border="1">
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,AE)}$</td> <td>time</td>
<tr><td> NN</td>      	 <td>1.00</td><td>0.60</td><td>0.53</td> <td>0.66</td><td>0.69</td> <td>0.620667</td></tr>
<tr><td> Rerank</td>     <td>0.99</td><td>0.76</td><td>0.72</td> <td><span class="dotted-u">0.82</span></td><td>0.83</td><td>0.804068</td> </tr>
<tr><td> Greedy</td>     <td>0.95</td><td>0.75</td><td>0.71</td> <td>0.80</td><td>0.81</td><td>5.685634</td> </tr>
<tr><td> MMR</td>        <td>0.92</td><td>0.73</td><td>0.68</td> <td>0.77</td><td>0.78</td><td>5.167833</td> </tr>
<tr><td> QIP</td>   	 <td>1.00</td><td>0.74</td><td>0.69</td> <td><u>0.81</u></td><td>0.82</td> <td>704.9731</td></tr>
</table>
</td><td>
<table border="1">
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,AE)}$</td> <td>time</td>
<tr><td> ANN</td>      	 <td>0.98</td><td>0.76</td><td>0.74</td> <td>0.83</td><td>0.84</td><td>0.180617</td> </tr>
<tr><td> Rerank</td>     <td>0.95</td><td>0.79</td><td>0.76</td> <td><span class="dashed-u">0.84</span></td><td>0.85</td><td>0.154010</td> </tr>
<tr><td> Greedy</td>     <td>0.91</td><td>0.78</td><td>0.76</td> <td>0.82</td><td>0.82</td><td>0.985829</td> </tr>
<tr><td> MMR</td>        <td>0.92</td><td>0.78</td><td>0.74</td> <td>0.81</td><td>0.82</td><td>1.102152</td> </tr>
<tr><td> QIP</td>        <td>0.93</td><td>0.80</td><td>0.77</td> <td><span class="ridge-u">0.83</span></td><td>0.84</td><td>0.487790</td> </tr>

</table>
</td></tr>
</table>
<br/>
<table align="center" border="1">
<caption><b>Table 16.2</b>: Performance of baseline(left), svd(right) methods @ Top 20 </caption>
<tr><td>
<table border="1">
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,AE)}$</td> <td>time</td>
<tr><td> NN</td>      	 <td>0.99</td><td>0.72</td><td>0.60</td> <td>0.73</td><td>0.75</td> <td>0.720512</td></tr>
<tr><td> Rerank</td>     <td>0.99</td><td>0.84</td><td>0.75</td> <td><span class="dotted-u">0.85</span></td><td>0.86</td><td>0.792849</td> </tr>
<tr><td> Greedy</td>     <td>0.98</td><td>0.86</td><td>0.77</td> <td>0.85</td><td>0.86</td><td>11.192675</td> </tr>
<tr><td> MMR</td>        <td>0.95</td><td>0.86</td><td>0.75</td> <td>0.83</td><td>0.84</td><td>10.584732</td> </tr>
<tr><td> QIP</td>        <td>1.00</td><td>0.82</td><td>0.73</td> <td><u>0.84</u></td><td>0.84</td> <td>947.091</td> </tr>
</table>
</td><td>
<table border="1">
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,AE)}$</td> <td>time</td>
<tr><td> ANN</td>      	 <td>0.95</td><td>0.89</td><td>0.85</td> <td>0.89</td><td>0.90</td> <td>0.182820</td></tr>
<tr><td> Rerank</td>     <td>0.94</td><td>0.91</td><td>0.85</td> <td><span class="dashed-u">0.89</span></td><td>0.89</td><td>0.178973</td> </tr>
<tr><td> Greedy</td>     <td>0.69</td><td>0.88</td><td>0.80</td> <td>0.73</td><td>0.74</td> <td>2.417187</td></tr>
<tr><td> MMR</td>        <td>0.93</td><td>0.91</td><td>0.84</td> <td>0.88</td><td>0.88</td> <td>2.085060</td></tr>
<tr><td> QIP</td>        <td>0.92</td><td>0.94</td><td>0.86</td> <td><span class="ridge-u">0.88</span></td><td>0.89</td><td>0.499330</td> </tr>
</table>
</td></tr>
</table>
<br/>
<table align="center" border="1">
<caption><b>Table 16.3</b>: Performance of baseline(left), svd(right) methods @ Top 30 </caption>
<tr><td>
<table border="1">
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,AE)}$</td> <td>time</td>
<tr><td> NN</td>      	 <td>0.99</td><td>0.79</td><td>0.64</td> <td>0.77</td><td>0.78</td> <td>0.845159</td></tr>
<tr><td> Rerank</td>     <td>0.99</td><td>0.88</td><td>0.77</td> <td><span class="dotted-u">0.86</span></td><td>0.87</td><td>0.900628</td> </tr>
<tr><td> Greedy</td>     <td>0.97</td><td>0.90</td><td>0.79</td> <td>0.87</td><td>0.87</td> <td>17.161591</td></tr>
<tr><td> MMR</td>        <td>0.96</td><td>0.90</td><td>0.77</td> <td>0.84</td><td>0.85</td> <td>16.523978</td></tr>
<tr><td> QIP</td>        <td>1.00</td><td>0.87</td><td>0.76</td> <td><u>0.86</u></td><td>0.86</td><td>1137.19</td> </tr>
</table>
</td><td>
<table border="1">
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,AE)}$</td> <td>time</td>
<tr><td> ANN</td>      	 <td>0.89</td><td>0.98</td><td>0.91</td> <td>0.90</td><td>0.90</td> <td>0.105862</td></tr>
<tr><td> Rerank</td>     <td>0.87</td><td>0.98</td><td>0.90</td> <td><span class="dashed-u">0.88</span></td><td>0.88</td><td>0.202913</td> </tr>
<tr><td> Greedy</td>     <td>0.53</td><td>0.89</td><td>0.80</td> <td>0.62</td><td>0.64</td> <td>3.536949</td></tr>
<tr><td> MMR</td>        <td>0.87</td><td>0.97</td><td>0.90</td> <td>0.88</td><td>0.89</td> <td>4.105651</td></tr>
<tr><td> QIP</td>        <td>0.86</td><td>0.98</td><td>0.90</td> <td><span class="ridge-u">0.88</span></td><td>0.88</td><td>0.501941</td> </tr>
</table>
</td></tr>
</table>

<p align="center">---------------------------- <b> Update: 30/03/2015</b>  ----------------------------</p>

The quadratic optimization problem is convex problem if and only if the function $f(x) = x^{T}Qx$ is convex. Furthermore, the following statements are equivalent.
<ol>
<li> $f$ is convex. </li>
<li> $Q$ is positive semidefinite. </li>
<li> There exist a matrix $H$ such that $Q = HH^{T}$ </li>
</ol>

<p>In our case, the matrix $Q$ is by construction a positive semidefinite. Note that $H$ is not unique in general, for instance H may be the Cholesky factor or $Q^{1/2}$. Most optimization solvers (like MOSEK), is not informed about $H$ but is only given $Q$, therefore a convexity check is performed by computing a Cholesky factor of $Q$. Unfortunately, this is not a robust convexity check if the rounding errors are present as the rounded $Q$ may not be a positive semidefinite. Therefore, in practice, the wrong conclusions about the convexity cannot be ruled out. </p>

<b>Overcome the convexity check</b>: 
<ol>
<li> Compute the eigen decomposition of Q and cut off all near zero negative eigen values to zero. Do a backward computation to approximate $Q$ to $\hat{Q}$. Use $\hat{Q}$ in the objective. 
<li> Reformutation to seperable form may lead to much simpler and foolproof convexity check. However, this requires the knowledge of $H \in R^{n \times p}$. Given $H$, we can reformulate the quadratic optimization problem to a quadratic conic optimization problem as follows: 
<p align="center">	Minimize $ \beta + c^{T}\alpha $</p>
<p align="center">	subject to $ \|H\alpha\|_{2} \leq \beta;$    $\alpha^{T}1 = k;$ and $\alpha \in Z^{n}, \beta \in R, Q = HH^{T}$ </p>
</ol>
	
<p align="center">---------------------------- <b> Update: 11/03/2015 - 24/03/2015</b>  ----------------------------</p>

<b>Exact formulation to diverse retrieval</b>: Given a database of images $X = [x_{1}, \ldots , x_{n}]; \forall i, x_{i} \in R^{d}$ and a hyperplane $w \in R^{d}$, our objective is to retrieve top-k images relevant to the query $w$ that satisfies the following:  
<p align="center">	Maximize $ \Sigma_{i=1}^{n}\alpha_{i}w^{T}x_{i} + \Sigma_{ij}\alpha_{i}\alpha_{j}\|x_{i} - x_{j}\|_{2}^{2}$ </p>
<p align="center">      subject to $ \Sigma_{i=1}^{n}\alpha_{i} = k;$ and $\alpha_{i} \in \{0,1\}$ </p>
Assuming that $x_{i}, w$ are normalized to unit norm, we can further write this as

<p align="center">	Maximize $ \Sigma_{i=1}^{n}\alpha_{i}w^{T}x_{i} - 2\Sigma_{ij}\alpha_{i}\alpha_{j}x_{i}^{T}x_{j}$ </p>
<p align="center">      subject to $ \Sigma_{i=1}^{n}\alpha_{i} = k;$ and $\alpha_{i} \in \{0,1\}$ </p>

Let $\alpha = [\alpha_{1}, \ldots \alpha_{n}]; u = [w^{T}x_{1}, \ldots, w^{T}x_{n}]$, $P$ be gram matrix with $P_{ij} = x_{i}^{T}x_{j}$ and $D$ be the diagonal matrix whose entries are $[P_{11},\ldots ,P_{nn}]$, then the above objective is equivalent to
	
<p align="center">	Maximize $ u^{T}\alpha - \alpha^{T}(P-D)\alpha$ </p>
<p align="center">      subject to $\alpha^{T}1 = k;$ and $\alpha \in Z^{n}$ </p>

With simple substitutions $c = -u$ and $Q = (P-D)$, we have
<p align="center">	Minimize $ \alpha^{T}Q\alpha + c^{T}\alpha $</p>
<p align="center">      subject to $\alpha^{T}1 = k;$ and $\alpha \in Z^{n}$ </p>

In general, $Q = P-D$ is symmetric but not a PSD matrix and so, we end up with a non-convex problem. Since, we are only interested in top-k retrieval, simply use $Q = P$ by dropping the diagonal matrix $D$ in the objective. Therefore, the diverse retrieval problem reduces to a quadratic optimization problem as   
<p align="center">	Minimize $ \alpha^{T}Q\alpha + c^{T}\alpha $</p>
<p align="center">      subject to $\alpha^{T}1 = k;$ and $\alpha \in Z^{n}$ </p>


<p align="center">---------------------------- <b> Update: 27/01/2015</b>  ----------------------------</p>

<table border="1" align="center">
<tr><td>
<table border="1">
<caption><b>NN @10</b></caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>1.00</td><td>0.60</td><td>0.85</td> <td>0.66</td><td>0.69</td> </tr>
<tr><td> Rerank</td>            <td>0.99</td><td>0.76</td><td>1.16</td> <td><span class="dotted-u">0.82</span></td><td>0.83</td> </tr>
<tr><td> Greedy</td>            <td>0.95</td><td>0.75</td><td>1.15</td> <td>0.80</td><td>0.81</td> </tr>
<tr><td> MMR</td>               <td>0.92</td><td>0.73</td><td>1.10</td> <td>0.77</td><td>0.78</td> </tr>
</table>
</td><td>
<table border="1">
<caption><b>LSH-Div @10</b></caption>
<tr><td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr>      	 <td>0.97</td><td>0.79</td><td>1.22</td> <td>0.84</td><td>0.85</td> </tr>
<tr>            <td>0.93</td><td>0.80</td><td>1.23</td> <td><span class="dashed-u">0.83</span></td><td>0.84</td> </tr>
<tr>           <td>0.89</td><td>0.80</td><td>1.23</td> <td>0.81</td><td>0.82</td> </tr>
<tr>           <td>0.91</td><td>0.77</td><td>1.18</td> <td>0.80</td><td>0.81</td> </tr>
</table>
</td><td>
<table border="1">
<caption><b>KDT-Div @10</b></caption>
<tr><td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr>      	 <td>0.91</td><td>0.57</td><td>0.50</td> <td>0.59</td><td>0.64</td> </tr>
<tr>            <td>0.85</td><td>0.73</td><td>0.72</td> <td><span class="dotted-u">0.76</span></td><td>0.77</td> </tr>
<tr>    <td>-</td><td>-</td><td>-</td> <td><span class="dashed-u">-</span></td><td>-</td> </tr>
<tr>               <td>0.86</td><td>0.73</td><td>0.71</td> <td>0.76</td><td>0.77</td> </tr>
<!--
<tr>      	 <td>0.88</td><td>0.57</td><td>0.51</td> <td>0.58</td><td>0.64</td> </tr>
<tr>           <td>0.80</td><td>0.72</td><td>0.70</td> <td><span class="dotted-u">0.73</span></td><td>0.75</td> </tr>
<tr>    <td>-</td><td>-</td><td>-</td> <td><span class="dashed-u">-</span></td><td>-</td> </tr>
<tr>               <td>0.82</td><td>0.73</td><td>0.72</td> <td>0.75</td><td>0.76</td> </tr>
-->
<!--<tr>    <td>1.00</td><td>0.61</td><td>0.55</td> <td>0.68</td><td>0.71</td></tr>
<tr>	<td>0.99</td><td>0.80</td><td>0.77</td> <td><span class="dotted-u">0.86</span></td><td>0.87</td> </tr>
<tr>    <td>-</td><td>-</td><td>-</td> <td><span class="dashed-u">-</span></td><td>-</td> </tr>
<tr>	<td>0.99</td><td>0.80</td><td>0.77</td> <td>0.86</td><td>0.87</td> </tr>-->
</table>
</td><td>

<table align="center" border="1">
<caption><b>Time @10</b></caption>
<tr>	<td>baseline </td> <td>LSH </td>    <td>SVD</td> <td>Kd-Tree</td></tr>
<tr>    <td>0.620667</td> <td>0.112893</td> <td>0.180617</td><td>0.110608<!--1.554186--></td> </tr>
<tr>    <td>0.804068</td> <td>0.141528</td> <td>0.154010</td><td>0.210086</td> </tr>
<tr>    <td>5.685634</td> <td>1.265356</td> <td>0.985829</td><td></td> </tr>
<tr>    <td>5.167833</td> <td>1.134540</td> <td>1.102152</td><td></td> </tr>
</table>

</td></tr>
</table>
<br>
<table border="1" align="center">
<tr><td>
<table border="1">
<caption><b>NN @20</b></caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.99</td><td>0.72</td><td>0.97</td> <td>0.73</td><td>0.75</td> </tr>
<tr><td> Rerank</td>            <td>0.99</td><td>0.84</td><td>1.21</td> <td><span class="dotted-u">0.85</span></td><td>0.86</td> </tr>
<tr><td> Greedy</td>            <td>0.98</td><td>0.86</td><td>1.24</td> <td>0.85</td><td>0.86</td> </tr>
<tr><td> MMR</td>               <td>0.95</td><td>0.86</td><td>1.21</td> <td>0.83</td><td>0.84</td> </tr>
</table>
</td><td>
<table border="1">
<caption><b>LSH-Div @20</b></caption>
<tr><td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr>      	 <td>0.93</td><td>0.93</td><td>1.38</td> <td>0.89</td><td>0.89</td> </tr>
<tr>            <td>0.92</td><td>0.93</td><td>1.38</td> <td><span class="dashed-u">0.88</span></td><td>0.89</td> </tr>
<tr>            <td>0.68</td><td>0.88</td><td>1.30</td> <td>0.72</td><td>0.73</td> </tr>
<tr>         <td>0.91</td><td>0.92</td><td>1.37</td> <td>0.87</td><td>0.88</td> </tr>
</table>
</td><td>
<table border="1">
<caption><b>KDT-Div @20</b></caption>
<tr><td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr>      	 <td>0.90</td><td>0.70</td><td>0.57</td> <td>0.65</td><td>0.70</td> </tr>
<tr>            <td>0.86</td><td>0.85</td><td>0.78</td> <td><span class="dotted-u">0.79</span></td><td>0.81</td> </tr>
<tr>            <td>-</td><td>-</td><td>-</td> <td><span class="dashed-u">-</span></td><td>-</td> </tr>
<tr>               <td>0.86</td><td>0.86</td><td>0.77</td> <td>0.79</td><td>0.81</td> </tr>
<!--
<tr>      	 <td>0.83</td><td>0.71</td><td>0.60</td> <td>0.63</td><td>0.69</td> </tr>
<tr>           <td>0.79</td><td>0.84</td><td>0.78</td> <td><span class="dotted-u">0.76</span></td><td>0.78</td> </tr>
<tr>            <td>-</td><td>-</td><td>-</td> <td><span class="dashed-u">-</span></td><td>-</td> </tr>
<tr>               <td>0.80</td><td>0.84</td><td>0.77</td> <td>0.76</td><td>0.79</td> </tr>
-->
<!--<tr> <td>1.00</td><td>0.73</td><td>0.62</td> <td>0.74</td><td>0.76</td></tr>
<tr> <td>0.99</td><td>0.85</td><td>0.80</td> <td><span class="dotted-u">0.88</span></td><td>0.88</td> </tr>
<tr>            <td>-</td><td>-</td><td>-</td> <td><span class="dashed-u">-</span></td><td>-</td> </tr>
<tr>	<td>0.99</td><td>0.87</td><td>0.80</td> <td>0.88</td><td>0.88</td> </tr>-->
</table>
</td><td>

<table align="center" border="1">
<caption><b>Time @20</b></caption>
<tr>	<td>baseline </td> <td>LSH </td>    <td>SVD</td> <td>Kd-Tree</td></tr>
<tr>    <td>0.720512</td> <td>0.136627</td> 	<td>0.182820</td><td>0.181632<!--1.782360--></td></tr>
<tr>    <td>0.792849</td> <td>0.146277</td> 	<td>0.178973</td><td>0.196505</td></tr>
<tr>    <td>11.192675</td> <td>2.392025</td> 	<td>2.417187</td></tr>
<tr>    <td>10.584732</td> <td>2.377893</td> 	<td>2.085060</td></tr>
</table>

</td></tr>
</table>
<br>
<table border="1" align="center">
<tr><td>
<table border="1">
<caption><b>NN @30</b></caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.99</td><td>0.79</td><td>1.04</td> <td>0.77</td><td>0.78</td> </tr>
<tr><td> Rerank</td>            <td>0.99</td><td>0.88</td><td>1.24</td> <td><span class="dotted-u">0.86</span></td><td>0.87</td> </tr>
<tr><td> Greedy</td>            <td>0.97</td><td>0.90</td><td>1.28</td> <td>0.87</td><td>0.87</td> </tr>
<tr><td> MMR</td>               <td>0.96</td><td>0.90</td><td>1.23</td> <td>0.84</td><td>0.85</td> </tr>
</table>
</td><td>
<table border="1">
<caption><b>LSH-Div @30</b></caption>
<tr><td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr>      	 <td>0.89</td><td>0.98</td><td>1.47</td> <td>0.90</td><td>0.90</td> </tr>
<tr>            <td>0.87</td><td>0.98</td><td>1.45</td> <td><span class="dashed-u">0.88</span></td><td>0.88</td> </tr>
<tr>           <td>0.53</td><td>0.89</td><td>1.29</td> <td>0.62</td><td>0.64</td> </tr>
<tr>               <td>0.87</td><td>0.97</td><td>1.44</td> <td>0.88</td><td>0.89</td> </tr>
</table>
</td><td>
<table border="1">
<caption><b>KDT-Div @30</b></caption>
<tr><td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr>    	 <td>0.89</td><td>0.79</td><td>0.61</td> <td>0.68</td><td>0.73</td> </tr>
<tr>            <td>0.85</td><td>0.90</td><td>0.79</td> <td><span class="dotted-u">0.79</span></td><td>0.82</td> </tr>
<tr>		 <td>-</td><td>-</td><td>-</td> <td><span class="dashed-u">-</span></td><td>-</td> </tr>
<tr>               <td>0.86</td><td>0.91</td><td>0.78</td> <td>0.79</td><td>0.82</td> </tr>

<!--<tr>     	 <td>0.81</td><td>0.80</td><td>0.64</td> <td>0.65</td><td>0.72</td> </tr>
<tr>             <td>0.77</td><td>0.88</td><td>0.79</td> <td><span class="dotted-u">0.75</span></td><td>0.78</td> </tr>
<tr>		 <td>-</td><td>-</td><td>-</td> <td><span class="dashed-u">-</span></td><td>-</td> </tr>
<tr>             <td>0.79</td><td>0.89</td><td>0.78</td> <td>0.75</td><td>0.79</td> </tr>
-->
<!--<tr>    <td>1.00</td><td>0.80</td><td>0.66</td> <td>0.78</td><td>0.79</td></tr>
<tr>	<td>0.99</td><td>0.88</td><td>0.81</td> <td><span class="dotted-u">0.89</span></td><td>0.89</td> </tr>
<tr>    <td>-</td><td>-</td><td>-</td> <td><span class="dashed-u">-</span></td><td>-</td> </tr>
<tr>    <td>0.99</td><td>0.90</td><td>0.79</td> <td>0.88</td><td>0.88</td> </tr>-->
</table>
</td><td>

<table align="center" border="1">
<caption><b>Time @30</b></caption>
<tr>	<td>baseline </td> <td>LSH </td>    <td>SVD</td> <td>Kd-Tree</td></tr>
<tr>    <td>0.845159</td> <td>0.178645</td> 	<td>0.105862</td><td>0.108348<!--1.609333--></td></tr>
<tr>    <td>0.900628</td> <td>0.214144</td> 	<td>0.202913</td><td>0.295774</td></tr>
<tr>    <td>17.161591</td> <td>4.437053</td> 	<td>3.536949</td></tr>
<tr>    <td>16.523978</td> <td>3.827656</td> 	<td>4.105651</td></tr>
</table>
</td></tr>
</table>

<p align="center">---------------------------- <b> Update: 05/11/2014</b>  ----------------------------</p>

Related work in diverse NN search: 
<ul>
<li>Diverse Near Neighbor Problem, SoCG, 2013 <a href="http://www.mit.edu/~mahabadi/KDNN.pdf">[pdf]</a></li>
<li>Composable core-sets for diversity and coverage maximization, PODS, 2014   <a href="http://www.mit.edu/~mahabadi/coreset.pdf">[pdf]</a></li>
<li>$\lambda$-diverse nearest neighbors browsing for multidimensional data, KDE, 2013 <a href="http://www.dblab.ntua.gr/~gtsat/collection/diversity/book.pdf">[pdf]</a></li>
<li>The KNDN Problem: A Quest for Unity in Diversity <a href="http://sites.computer.org/debull/A09dec/haritsa-paper1.pdf">[pdf]</a></li>
</ul>

<p align="center">---------------------------- <b> Update: 23/09/2014</b>  ----------------------------</p>

<table align="center" border="1">
<caption><b>Table 15</b>: Retrieval time(in sec) for various methods</caption>
<tr><td>
<table align="center" border="1">
<caption><b>Top 5</b></caption>
<tr><td><b> Methods</b></td>	<td>baseline </td> <td>LSH </td>   <td>SVD</td></tr>
<tr><td><b> Naive</b></td>     <td>0.570013</td> <td>0.096191</td> <td>0.118293</td> </tr>
<tr><td><b> Rerank</b></td>    <td>0.763124</td> <td>0.147858</td> <td>0.145353</td> </tr>
<tr><td><b> Greedy</b></td>    <td>2.941714</td> <td>0.594480</td> <td>0.745475</td> </tr>
<tr><td><b> MMR</b></td>       <td>2.417155</td> <td>0.648030</td> <td>0.976767</td> </tr>
<!--<tr><td><b> DPP</b></td>  	<td></td> <td></td> </tr>-->
</table>
</td>
<td>
<table align="center" border="1">
<caption><b>Top 10</b></caption>
<tr>	<td>baseline </td> <td>LSH </td>    <td>SVD</td></tr>
<tr>    <td>0.620667</td> <td>0.112893</td> <td>0.180617</td> </tr>
<tr>    <td>0.804068</td> <td>0.141528</td> <td>0.154010</td> </tr>
<tr>    <td>5.685634</td> <td>1.265356</td> <td>0.985829</td> </tr>
<tr>    <td>5.167833</td> <td>1.134540</td> <td>1.102152</td> </tr>
<!--<tr>    <td>1.350894</td> <td>0.207628</td> <td></td> </tr>-->
</table>
 </td>
<td> 
<table align="center" border="1">
<caption><b>Top 20</b></caption>
<tr>	<td>baseline </td><td>LSH </td>	<td>SVD</td></tr>
<tr>    <td>0.720512</td> <td>0.136627</td> 	<td>0.182820</td></tr>
<tr>    <td>0.792849</td> <td>0.146277</td> 	<td>0.178973</td></tr>
<tr>    <td>11.192675</td> <td>2.392025</td> 	<td>2.417187</td></tr>
<tr>    <td>10.584732</td> <td>2.377893</td> 	<td>2.085060</td></tr>
<!--<tr>  	<td> . </td> <td> </td> </tr>-->
</table>
</td>
<td>
<table align="center" border="1">
<caption><b>Top 30</b></caption>
<tr>	<td>baseline </td><td>LSH </td>	<td>SVD</td></tr>
<tr>    <td>0.845159</td> <td>0.178645</td> 	<td>0.105862</td></tr>
<tr>    <td>0.900628</td> <td>0.214144</td> 	<td>0.202913</td></tr>
<tr>    <td>17.161591</td> <td>4.437053</td> 	<td>3.536949</td></tr>
<tr>    <td>16.523978</td> <td>3.827656</td> 	<td>4.105651</td></tr>
<!--<tr>  	<td> . </td> <td> </td> </tr>-->
</table>
</td>
</tr>
</table>

<br/>

<table align="center" border="1">
<caption><b>Table 14.1</b>: Performance of baseline(left), lsh(middle) and svd(right) methods @ Top 5 </caption>
<tr><td>
<table border="1">
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.99</td><td>0.47</td><td>0.70</td> <td>0.57</td><td>0.61</td> </tr>
<tr><td> Rerank</td>            <td>1.00</td><td>0.61</td><td>1.01</td> <td><span class="dotted-u">0.76</span></td><td>0.77</td> </tr>
<tr><td> Greedy</td>            <td>0.90</td><td>0.58</td><td>0.96</td> <td>0.69</td><td>0.72</td> </tr>
<tr><td> MMR</td>               <td>0.83</td><td>0.54</td><td>0.86</td> <td>0.62</td><td>0.65</td> </tr>
<!--<tr><td> DPP</td>               <td>1.00</td><td>0.52</td><td>0.69</td> <td><u>0.57</u></td><td>0.60</td> </tr>-->
</table>
</td><td>
<table border="1">
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.98</td><td>0.58</td><td>0.93</td> <td>0.70</td><td>0.73</td> </tr>
<tr><td> Rerank</td>            <td>0.91</td><td>0.60</td><td>0.99</td> <td><span class="dashed-u">0.72</span></td><td>0.74</td> </tr>
<tr><td> Greedy</td>            <td>0.83</td><td>0.57</td><td>0.93</td> <td>0.66</td><td>0.68</td> </tr>
<tr><td> MMR</td>               <td>0.86</td><td>0.55</td><td>0.87</td> <td>0.64</td><td>0.66</td> </tr>
<!--<tr><td> DPP</td>               <td>0.97</td><td>0.81</td><td>1.25</td> <td><span class="ridge-u">0.85</span></td><td>0.86</td> </tr>-->
</table>
</td><td>
<table border="1">
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.98</td><td>0.55</td><td>0.89</td> <td>0.68</td><td>0.71</td> </tr>
<tr><td> Rerank</td>            <td>0.93</td><td>0.60</td><td>0.98</td> <td><span class="dashed-u">0.72</span></td><td>0.74</td> </tr>
<tr><td> Greedy</td>            <td>0.83</td><td>0.56</td><td>0.93</td> <td>0.67</td><td>0.68</td> </tr>
<tr><td> MMR</td>               <td>0.85</td><td>0.54</td><td>0.86</td> <td>0.63</td><td>0.66</td> </tr>
<!--<tr><td> DPP</td>               <td>0.97</td><td>0.81</td><td>1.25</td> <td><span class="ridge-u">0.85</span></td><td>0.86</td> </tr>-->
</table>
</td></tr>
</table>
<br/>

<table align="center" border="1">
<caption><b>Table 14.2</b>: Performance of baseline(left), lsh(middle) and svd(right) methods @ Top 10 </caption>
<tr><td>
<table border="1">
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>1.00</td><td>0.60</td><td>0.85</td> <td>0.66</td><td>0.69</td> </tr>
<tr><td> Rerank</td>            <td>0.99</td><td>0.76</td><td>1.16</td> <td><span class="dotted-u">0.82</span></td><td>0.83</td> </tr>
<tr><td> Greedy</td>            <td>0.95</td><td>0.75</td><td>1.15</td> <td>0.80</td><td>0.81</td> </tr>
<tr><td> MMR</td>               <td>0.92</td><td>0.73</td><td>1.10</td> <td>0.77</td><td>0.78</td> </tr>
<!--<tr><td> DPP</td>               <td>0.99</td><td>0.61</td><td>0.88</td> <td><u>0.68</u></td><td>0.71</td> </tr>-->
</table>
</td><td>
<table border="1">
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.97</td><td>0.79</td><td>1.22</td> <td>0.84</td><td>0.85</td> </tr>
<tr><td> Rerank</td>            <td>0.93</td><td>0.80</td><td>1.23</td> <td><span class="dashed-u">0.83</span></td><td>0.84</td> </tr>
<tr><td> Greedy</td>            <td>0.89</td><td>0.80</td><td>1.23</td> <td>0.81</td><td>0.82</td> </tr>
<tr><td> MMR</td>               <td>0.91</td><td>0.77</td><td>1.18</td> <td>0.80</td><td>0.81</td> </tr>
<!--<tr><td> DPP</td>               <td>0.95</td><td>0.82</td><td>1.26</td> <td><span class="ridge-u">0.85</span></td><td>0.86</td> </tr>-->
</table>
</td><td>
<table border="1">
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.98</td><td>0.76</td><td>1.18</td> <td>0.83</td><td>0.84</td> </tr>
<tr><td> Rerank</td>            <td>0.95</td><td>0.79</td><td>1.23</td> <td><span class="dashed-u">0.84</span></td><td>0.85</td> </tr>
<tr><td> Greedy</td>            <td>0.91</td><td>0.78</td><td>1.22</td> <td>0.82</td><td>0.82</td> </tr>
<tr><td> MMR</td>               <td>0.92</td><td>0.78</td><td>1.20</td> <td>0.81</td><td>0.82</td> </tr>
<!--<tr><td> DPP</td>               <td>0.97</td><td>0.81</td><td>1.25</td> <td><span class="ridge-u">0.85</span></td><td>0.86</td> </tr>-->
</table>
</td></tr>
</table>
<br/><br/>
<table align="center" border="1">
<caption><b>Table 14.3</b>: Performance of baseline(left),  lsh(middle) and svd(right) methods @ Top 20 </caption>
<tr><td>
<table border="1">
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.99</td><td>0.72</td><td>0.97</td> <td>0.73</td><td>0.75</td> </tr>
<tr><td> Rerank</td>            <td>0.99</td><td>0.84</td><td>1.21</td> <td><span class="dotted-u">0.85</span></td><td>0.86</td> </tr>
<tr><td> Greedy</td>            <td>0.98</td><td>0.86</td><td>1.24</td> <td>0.85</td><td>0.86</td> </tr>
<tr><td> MMR</td>               <td>0.95</td><td>0.86</td><td>1.21</td> <td>0.83</td><td>0.84</td> </tr>
<!--<tr><td> DPP</td>               <td>1.00</td><td>0.52</td><td>0.69</td> <td><u>0.57</u></td><td>0.60</td> </tr>-->
</table>
</td><td>
<table border="1">
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.93</td><td>0.93</td><td>1.38</td> <td>0.89</td><td>0.89</td> </tr>
<tr><td> Rerank</td>            <td>0.92</td><td>0.93</td><td>1.38</td> <td><span class="dashed-u">0.88</span></td><td>0.89</td> </tr>
<tr><td> Greedy</td>            <td>0.68</td><td>0.88</td><td>1.30</td> <td>0.72</td><td>0.73</td> </tr>
<tr><td> MMR</td>               <td>0.91</td><td>0.92</td><td>1.37</td> <td>0.87</td><td>0.88</td> </tr>
<!--<tr><td> DPP</td>               <td>0.97</td><td>0.81</td><td>1.25</td> <td><span class="ridge-u">0.85</span></td><td>0.86</td> </tr>-->
</table>
</td><td>
<table border="1">
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.95</td><td>0.89</td><td>1.36</td> <td>0.89</td><td>0.90</td> </tr>
<tr><td> Rerank</td>            <td>0.94</td><td>0.91</td><td>1.37</td> <td><span class="dashed-u">0.89</span></td><td>0.89</td> </tr>
<tr><td> Greedy</td>            <td>0.69</td><td>0.88</td><td>1.29</td> <td>0.73</td><td>0.74</td> </tr>
<tr><td> MMR</td>               <td>0.93</td><td>0.91</td><td>1.35</td> <td>0.88</td><td>0.88</td> </tr>
<!--<tr><td> DPP</td>               <td>0.97</td><td>0.81</td><td>1.25</td> <td><span class="ridge-u">0.85</span></td><td>0.86</td> </tr>-->
</table>
</td></tr>
</table>

<br/><br/>
<table align="center" border="1">
<caption><b>Table 14.4</b>: Performance of baseline(left),  lsh(middle) and svd(right) methods @ Top 30 </caption>
<tr><td>
<table border="1">
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.99</td><td>0.79</td><td>1.04</td> <td>0.77</td><td>0.78</td> </tr>
<tr><td> Rerank</td>            <td>0.99</td><td>0.88</td><td>1.24</td> <td><span class="dotted-u">0.86</span></td><td>0.87</td> </tr>
<tr><td> Greedy</td>            <td>0.97</td><td>0.90</td><td>1.28</td> <td>0.87</td><td>0.87</td> </tr>
<tr><td> MMR</td>               <td>0.96</td><td>0.90</td><td>1.23</td> <td>0.84</td><td>0.85</td> </tr>
<!--<tr><td> DPP</td>               <td>1.00</td><td>0.52</td><td>0.69</td> <td><u>0.57</u></td><td>0.60</td> </tr>-->
</table>
</td><td>
<table border="1">
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.89</td><td>0.98</td><td>1.47</td> <td>0.90</td><td>0.90</td> </tr>
<tr><td> Rerank</td>            <td>0.87</td><td>0.98</td><td>1.45</td> <td><span class="dashed-u">0.88</span></td><td>0.88</td> </tr>
<tr><td> Greedy</td>            <td>0.53</td><td>0.89</td><td>1.29</td> <td>0.62</td><td>0.64</td> </tr>
<tr><td> MMR</td>               <td>0.87</td><td>0.97</td><td>1.44</td> <td>0.88</td><td>0.89</td> </tr>
<!--<tr><td> DPP</td>               <td>0.97</td><td>0.81</td><td>1.25</td> <td><span class="ridge-u">0.85</span></td><td>0.86</td> </tr>-->
</table>
</td><td>
<table border="1">
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.92</td><td>0.95</td><td>1.43</td> <td>0.90</td><td>0.90</td> </tr>
<tr><td> Rerank</td>            <td>0.90</td><td>0.95</td><td>1.42</td> <td><span class="dashed-u">0.89</span></td><td>0.89</td> </tr>
<tr><td> Greedy</td>            <td>0.52</td><td>0.88</td><td>1.29</td> <td>0.61</td><td>0.63</td> </tr>
<tr><td> MMR</td>               <td>0.89</td><td>0.96</td><td>1.41</td> <td>0.88</td><td>0.88</td> </tr>
<!--<tr><td> DPP</td>               <td>0.97</td><td>0.81</td><td>1.25</td> <td><span class="ridge-u">0.85</span></td><td>0.86</td> </tr>-->
</table>
</td></tr>
</table>

<p align="center">---------------------------- <b> Update: 27/05/2014</b>  ----------------------------</p>

Data dependent LSH methods.
<ul>
<li>Density Sensitive Hashing, arXiv, 2012 [<a href="http://arxiv.org/abs/1205.2930">link</a>]</li>
<li>Approximate Analytics: Keeping Pace with Big Data using Parallel Locality Sensitive Hashing, ISTC Big Data, 2013 [<a href="http://istc-bigdata.org/index.php/approximate-analytics-keeping-pace-with-big-data-using-parallel-locality-sensitive-hashing">link</a>]</li>
<li>Distribution-Aware Locality Sensitive Hashing, Advances in Multimedia Modeling, 2013 [<a href="http://link.springer.com/chapter/10.1007%2F978-3-642-35728-2_38">link</a>]</li> 

</ul>
SVM classifiers for image retrieval.
<ul>
<!--<li><a href="http://www.informatik.uni-trier.de/~ley/pers/hd/d/Drosou:Marina">here</a></li>-->
<li>Image Retrieval Using Eigen Queries, ACCV, 2012 [<a href="http://link.springer.com/chapter/10.1007/978-3-642-37444-9_36">link</a>]</li>
</ul>

<p align="center">---------------------------- <b> Update: 28/02/2014 - 07/03/2014</b>  ----------------------------</p>
<b>Designing LSH functions: </b><br>
<ul> 
<li><i>Cluster based Method</i>: From the training data, $X = \{x_{1}, \ldots, x_{n}\}$, get the Cluster centriods $C = \{c_{1}, \ldots, c_{k}\}$, ($c_{i} \in R^{d}$, $k$ is the number of random permutations required for LSH).</li>
<li><i>Singular values Method</i>: Take $k$ largest singular values from the training samples, $X$.</li>
</ul>

<table align="center" border="1">
<caption>Mean P, S, AE, $h_{(P,(AE/Z))}$ for 50 queries  using singular values approach</caption>
<tr><td>
<table align="center" border="1">
<caption><b>Table 13.1</b>: top 35 singular values </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.97</td><td>0.76</td><td>1.19</td> <td>0.83</td><td>0.84</td> </tr>
<tr><td> Rerank</td>            <td>0.96</td><td>0.79</td><td>1.22</td> <td><span class="dashed-u">0.84</span></td><td>0.85</td> </tr>
<tr><td> Greedy</td>            <td>0.92</td><td>0.80</td><td>1.23</td> <td>0.83</td><td>0.83</td> </tr>
<tr><td> MMR</td>               <td>0.94</td><td>0.77</td><td>1.19</td> <td>0.82</td><td>0.83</td> </tr>
</table>
</td>
<td>
<table align="center" border="1">
<caption><b>Table 13.2</b>: top 70 singular values  </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	<td>1.00</td><td>0.73</td><td>1.07</td> <td>0.78</td><td>0.80</td> </tr>
<tr><td> Rerank</td>            <td>0.99</td><td>0.77</td><td>1.15</td> <td><span class="dashed-u">0.82</span></td><td>0.83</td> </tr>
<tr><td> Greedy</td>            <td>0.95</td><td>0.78</td><td>1.17</td> <td>0.82</td><td>0.83</td> </tr>
<tr><td> MMR</td>               <td>0.97</td><td>0.74</td><td>1.11</td> <td>0.79</td><td>0.81</td> </tr>
</table>
</td></tr>
<tr><td>
<table align="center" border="1">
<caption><b>Table 13.3</b>: top 140 singular values </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	<td>0.99</td><td>0.77</td><td>1.15</td> <td>0.81</td><td>0.83</td> </tr>
<tr><td> Rerank</td>            <td>0.97</td><td>0.80</td><td>1.22</td> <td><span class="dashed-u">0.84</span></td><td>0.85</td> </tr>
<tr><td> Greedy</td>            <td>0.95</td><td>0.80</td><td>1.21</td> <td>0.83</td><td>0.84</td> </tr>
<tr><td> MMR</td>               <td>0.97</td><td>0.77</td><td>1.16</td> <td>0.82</td><td>0.83</td> </tr>
</table>
</td>
<td>
<table align="center" border="1">
<caption><b>Table 13.4</b>: top 187 singular values  </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.99</td><td>0.74</td><td>1.11</td> <td>0.80</td><td>0.81</td> </tr>
<tr><td> Rerank</td>            <td>0.97</td><td>0.79</td><td>1.20</td> <td><span class="dashed-u">0.83</span></td><td>0.85</td> </tr>
<tr><td> Greedy</td>            <td>0.95</td><td>0.83</td><td>1.27</td> <td>0.85</td><td>0.86</td> </tr>
<tr><td> MMR</td>               <td>0.97</td><td>0.79</td><td>1.20</td> <td>0.83</td><td>0.84</td> </tr>
</table>
</td>
</tr>
</table>
<br>

<table align="center" border="1">
<caption>Mean P, S, AE, $h_{(P,(AE/Z))}$ for 50 queries using clustering approach</caption>
<tr><td>
<table align="center" border="1">
<caption><b>Table 14.1</b>: #clusters = 35 </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.79</td><td>0.57</td><td>0.93</td> <td>0.64</td><td>0.67</td> </tr>
<tr><td> Rerank</td>            <td>0.74</td><td>0.59</td><td>0.95</td> <td><span class="dashed-u">0.64</span></td><td>0.66</td> </tr>
<tr><td> Greedy</td>            <td>0.70</td><td>0.61</td><td>1.00</td> <td>0.64</td><td>0.66</td> </tr>
<tr><td> MMR</td>               <td>0.74</td><td>0.58</td><td>0.92</td> <td>0.62</td><td>0.64</td> </tr>
</table>
</td>
<td>
<table align="center" border="1">
<caption><b>Table 14.2</b>: #clusters = 70 </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
</table>
</td></tr>
<tr><td>
<table align="center" border="1">
<caption><b>Table 14.3</b>: #clusters = 140  </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
</table>
</td>
<td>
<table align="center" border="1">
<caption><b>Table 14.4</b>: #clusters = 187  </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
</table>
</td>
</tr>
</table>
<br>
<p align="center">---------------------------- <b> Update: 07/02/2014 - 13/02/2014</b>  ----------------------------</p>

<b>LSH for Diverse image retrieval: </b>
<ol>
	<li> Given a database of points $X = \{x_{1}, \ldots, x_{n}\}$ with each $x_{i} \in R^{d}$, obtain $k$ random points from X as $C \subset X$, (k << n, $C$ a $k \times d$ matrix).</li>
	<li> To construct the LSH partitions, choose a random vector $r \in R^{k}$ and compute $r_{i} = r^{T}C$.</li> 
	<li> Use hash functions $h_{r_{1}} \ldots h_{r_{k}}$ for LSH as 
			$h_{\vec{r_{i}}}(\vec{x})= 
  				\begin{cases}
				   1 & \text{if } \vec{r_{i}}.\vec{x} \geq 0 \\
				   0 & \text{if } \vec{r_{i}}.\vec{x} < 0
				 \end{cases}$
	</li> 
</ol>
<table align="center" border="1">
<caption><b>Table 12</b>: diversity preserving lsh </caption>
<tr><td> $k$ </td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> 35</td>      	 <td>0.86</td><td>0.71</td><td>1.09</td> <td>0.74</td><td>0.76</td> </tr>  
<tr><td> 50</td>      	 <td>0.88</td><td>0.75</td><td>1.17</td> <td>0.79</td><td>0.80</td> </tr>
<tr><td> 100</td>       <td>0.83</td><td>0.66</td><td>1.00</td> <td>0.68</td><td>0.71</td> </tr> 
</table>
<p align="center">---------------------------- <b> Update: 24/01/2014 - 31/01/2014</b>  ----------------------------</p>

<table align="center" border="1">
<caption><b>Table 10</b>: Performance of greedy solution to DPP method for various kernels</caption>
<tr> <td align="center">Kernel</td><td>Results</td><td align="center">Empirical Analysis</td></tr>
<tr> <td>$K_{ij}^{1} = \frac{x_{i}^{T}x_{j}}{(|x_{i}-w||x_{j}-w|)^{d}}$ </td><td><a href="./dpp_greedyresults_kernel1.html">kernel1</a></li></td><td>Baseline performs better than lsh method</td></tr>
<tr> <td>$K_{ij}^{2} = \frac{x_{i}^{T}x_{j}}{(|w^{T}x_{i}||w^{T}x_{j}|)^{d}}$ </td><td> <a href="./dpp_greedyresults_kernel2.html">kernel2</a> </td><td>Baseline and LSH methods perform equally for approxpriate $d$</td></tr>
<tr> <td>$K_{ij}^{3} = \frac{x_{i}^{T}x_{j}}{e^{-\gamma(|w^{T}x_{i}|+|w^{T}x_{j}|)}}$ </td><td><a href="./dpp_greedyresults_kernel3.html">kernel3</a> </td><td>LSH performs better than Baseline <br> Precision is consistent for lsh methods and reasonably good.</td></tr>
<tr> <td>$K_{ij}^{4} = \frac{x_{i}^{T}x_{j}}{e^{-\gamma(|w^{T}x_{i}||w^{T}x_{j}|)}}$ </td><td><a href="./dpp_greedyresults_kernel4.html">kernel4</a> </td><td>LSH performs better than baseline using only top-50 and top-100. <br>Huge drop in precision for $top \ge 150$ </td></tr>
</table>
<br>
<table align="center" border="1">
<caption><b>Table 11</b>:Comparison with earlier results with greedy solution to DDP using $K_{ij}^{3}$ </caption>
<tr><td>
<table align="center" border="1">
<caption><b>Table 11.1</b>: Baseline </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>1.00</td><td>0.50</td><td>0.65</td> <td>0.55</td><td>0.58</td> </tr>
<tr><td> Rerank</td>    <td>1.00</td><td>0.73</td><td>1.11</td> <td><span class="dotted-u">0.81</span></td><td>0.82</td> </tr>
<tr><td> Greedy</td>            <td>0.99</td><td>0.74</td><td>1.10</td> <td>0.79</td><td>0.81</td> </tr>
<tr><td> MMR</td>               <td>0.97</td><td>0.72</td><td>1.06</td> <td>0.77</td><td>0.78</td> </tr>
<tr><td>DPP with $K_{ij}^{3}$</td>  <td>1.00</td><td>0.64</td><td>0.96</td> <td><u>0.74</u></td><td>0.75</td> </tr>
</table>
</td>
<td>
<table align="center" border="1">
<caption><b>Table 11.2</b>: LSH </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.98</td><td>0.81</td><td>1.23</td> <td>0.85</td><td>0.86</td> </tr>
<tr><td> Rerank</td>     	<td>0.96</td><td>0.82</td><td>1.26</td> <td>0.86</td><td>0.86</td> </tr> 
<tr><td> Greedy</td>            <td>0.92</td><td>0.81</td><td>1.24</td> <td>0.83</td><td>0.84</td> </tr>
<tr><td> MMR</td>               <td>0.96</td><td>0.79</td><td>1.20</td> <td>0.83</td><td>0.84</td> </tr>
<tr><td> DPP with $K_{ij}^{3}$</td> <td>0.97</td><td>0.83</td><td>1.27</td> <td><u>0.86</u></td><td>0.87</td> </tr>
</table>
</td>
</tr>
</table>
<p align="center">---------------------------- <b> Update: 17/01/2014 - 23/01/2014</b>  ----------------------------</p>
<ul>
	<li> Corrections to Entropy measure are made</li>
	<li> Rerank and DPP performance using top 50 are shown below </li>
	<li> DPP results with modified kernel did not yield any significant improvements</li>
	<li> Results in Table 9.1 and 9.2 are averaged over 4(out of 7) classes from our datsets</li>
	<li>Multi label learning: <a href="http://researchweb.iiit.ac.in/~vidyadhar.rao/multi-label_learning.html"> here</a>
</ul>
<table align="center" border="1">
<caption>Mean P, S, AE, $h_{(P,(AE/Z))}$ for 50 queries  </caption>
<tr><td>
<table align="center" border="1">
<caption>Performance on Training dataset </caption>
<tr><td>
<table align="center" border="1">
<caption><b>Table 9.1</b>: Baseline methods </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>1.00</td><td>0.65</td><td>0.61</td> <td>0.74</td><td>0.76</td> </tr>
<tr><td> Rerank</td>            <td>1.00</td><td>0.89</td><td>0.86</td> <td>0.92</td><td>0.92</td></tr>
<tr><td> Greedy</td>            <td>1.00</td><td>0.92</td><td>0.87</td> <td>0.93</td><td>0.93</td> </tr>
<tr><td> MMR</td>               <td>1.00</td><td>0.92</td><td>0.87</td> <td>0.92</td><td>0.93</td></tr>
</table>
</td>
<td>
<table align="center" border="1">
<caption><b>Table 9.2</b>: LSH methods  </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.98</td><td>0.87</td><td>0.83</td> <td>0.90</td><td>0.90</td> </tr>
<tr><td> Rerank</td>            <td>0.99</td><td>0.91</td><td>0.87</td> <td>0.92</td><td>0.93</td></tr>
<tr><td> Greedy</td>            <td>0.96</td><td>0.88</td><td>0.83</td> <td>0.89</td><td>0.89</td> </tr>
<tr><td> MMR</td>               <td>0.99</td><td>0.94</td><td>0.90</td> <td>0.94</td><td>0.94</td></tr>
</table>
</td>
</tr>
</table>
</td></tr>
<tr><td>
<table align="center" border="1">
<caption><br>(<i>For visual comparison of baseline and lsh methods, see <a href="diversity_comparison.html">here</a></i>) <br> Performance on Testing dataset<br> </caption>
<tr><td>
<table align="center" border="1">
<caption><b>Table 9.3</b>: Baseline results are <a href="viewResults/Jan22_baselineResults.html">here</a></caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>1.00</td><td>0.50</td><td>0.65</td> <td>0.55</td><td>0.58</td> </tr>
<tr><td> Rerank</td>   <td>1.00</td><td>0.73</td><td>1.11</td> <td><span class="dotted-u">0.81</span></td><td>0.82</td> </tr>
<tr><td> Greedy</td>            <td>0.99</td><td>0.74</td><td>1.10</td> <td>0.79</td><td>0.81</td> </tr>
<tr><td> MMR</td>               <td>0.97</td><td>0.72</td><td>1.06</td> <td>0.77</td><td>0.78</td> </tr>
<tr><td> DPP</td>       <td>1.00</td><td>0.60</td><td>0.86</td> <td><u>0.68</u></td><td>0.70</td> </tr> 
</table>
</td>
<td>
<table align="center" border="1">
<caption><b>Table 9.4</b>: LSH results are <a href="viewResults/Jan22_lshResults.html">here</a>  </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.98</td><td>0.81</td><td>1.23</td> <td>0.85</td><td>0.86</td> </tr>
<tr><td> Rerank</td>    <td>0.96</td><td>0.82</td><td>1.26</td> <td><span class="dashed-u">0.86</span></td><td>0.86</td> </tr> 
<tr><td> Greedy</td>            <td>0.92</td><td>0.81</td><td>1.24</td> <td>0.83</td><td>0.84</td> </tr>
<tr><td> MMR</td>               <td>0.96</td><td>0.79</td><td>1.20</td> <td>0.83</td><td>0.84</td> </tr>
<tr><td> DPP</td>   <td>0.85</td><td>0.82</td><td>1.29</td> <td><span class="ridge-u">0.82</span></td><td>0.83</td> </tr>  
</table>
</td>
</tr>
</table>
</td></tr>
</table>
<br>


<p align="center">---------------------------- <b> Update: 11/01/2014 - 16/01/2014</b>  ----------------------------</p>

Experiments include DPP based retrieval methods. 

<table align="center" border="1">
<caption>P, S, AE and $h$ for 50 randomized runs  </caption>
<tr><td>
<table align="center" border="1">
<caption><b>Table 7.1</b>: Baseline methods </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>1.00</td><td>0.50</td><td>0.65</td> <td>0.55</td><td>0.58</td> </tr>
<tr><td> Greedy</td>            <td>0.99</td><td>0.78</td><td>1.15</td> <td>0.82</td><td>0.83</td> </tr>
<tr><td> MMR</td>               <td>0.91</td><td>0.69</td><td>0.85</td> <td>0.63</td><td>0.67</td> </tr>
</table>
</td>
<td>
<table align="center" border="1">
<caption><b>Table 7.2</b>: LSH methods  </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.98</td><td>0.81</td><td>1.09</td> <td>0.78</td><td>0.80</td> </tr>
<tr><td> Greedy</td>            <td>0.85</td><td>0.78</td><td>0.91</td> <td>0.62</td><td>0.68</td> </tr>
<tr><td> MMR</td>               <td>0.94</td><td>0.79</td><td>1.06</td> <td>0.75</td><td>0.78</td> </tr>
</table>
</td>
</tr>
</table>
<br>

<table align="center" border="1">
<tr>
<td>
<table align="center" border="1">
<caption><b>Table:8.1</b> Using Top 50  </caption>
<tr><td>
<table align="center" border="1">
<caption> Baseline methods </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td></tr>
<tr><td> Rerank</td> <td>1.00</td><td>0.68</td><td>0.98</td> <td><span class="dotted-u"> 0.74</span></td><td>0.76</td> </tr>
<tr><td> DPP</td>   <td>1.00</td><td>0.57</td><td>0.79</td> <td><u>0.64</u></td><td>0.66</td> </tr>  
</table>
</td>
<td>
<table align="center" border="1">
<caption>LSH methods  </caption>
<tr><td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr>      <td>0.96</td><td>0.82</td><td>1.00</td> <td><span class="dashed-u"> 0.72</span></td><td>0.75</td> </tr>
<tr>      <td>0.85</td><td>0.81</td><td>0.75</td> <td><span class="ridge-u"> 0.56</span></td><td>0.60</td> </tr> 

</table>
</td>
</tr>
</table>
</td>
<td>
<table align="center" border="1">
<caption><b>Table:8.2</b> Using Top 100  </caption>
<tr><td>
<table align="center" border="1">
<caption>Baseline methods </caption>
<tr><td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr>    <td>1.00</td><td>0.72</td><td>1.07</td> <td><span class="dotted-u"> 0.78</span></td><td>0.80</td> </tr> 
<tr>    <td>1.00</td><td>0.61</td><td>0.84</td> <td><u>0.66</u></td><td>0.68</td> </tr>    
</table>
</td>
<td>
<table align="center" border="1">
<caption> LSH methods  </caption>
<tr><td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr>         <td>0.95</td><td>0.81</td><td>0.98</td> <td><span class="dashed-u"> 0.71</span></td><td>0.74</td> </tr> 
<tr>         <td>0.67</td><td>0.76</td><td>0.88</td> <td><span class="ridge-u"> 0.56</span></td><td>0.60</td> </tr>  
</table>
</td>
</tr>
</table>
</td>
</tr>
<tr><td>
<table align="center" border="1">
<caption><b>Table:8.3</b> Using Top 150  </caption>
<tr><td>
<table align="center" border="1">
<caption>Baseline methods </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Rerank</td>   <td>1.00</td><td>0.74</td><td>1.11</td> <td><span class="dotted-u"> 0.80</span></td><td>0.82</td> </tr>          
<tr><td> DPP</td>      <td>1.00</td><td>0.61</td><td>0.84</td> <td><u>0.67</u></td><td>0.69</td> </tr> 
</table>
</td>
<td>
<table align="center" border="1">
<caption> LSH methods  </caption>
<tr>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr>    <td>0.94</td><td>0.81</td><td>0.96</td> <td><span class="dashed-u"> 0.69</span></td><td>0.73</td> </tr> 
<tr>    <td>0.58</td><td>0.70</td><td>1.03</td> <td><span class="ridge-u"> 0.56</span></td><td>0.61</td> </tr> 
</table>
</td>
</tr>
</table>
</td><td>
<table align="center" border="1">
<caption><b>Table:8.4</b> Using Top 200  </caption>
<tr><td>
<table align="center" border="1">
<caption>Baseline methods </caption>
<tr><td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr>  <td>1.00</td><td>0.74</td><td>1.12</td> <td><span class="dotted-u"> 0.81</span></td><td>0.82</td> </tr>  
<tr>  <td>1.00</td><td>0.71</td><td>1.02</td> <td><u>0.75</u></td><td>0.77</td> </tr> 
</table>
</td>
<td>
<table align="center" border="1">
<caption> LSH methods  </caption>
<tr>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr>    <td>0.94</td><td>0.81</td><td>0.95</td> <td><span class="dashed-u"> 0.69</span></td><td>0.73</td> </tr>  
<tr>    <td>0.17</td><td>0.30</td><td>1.32</td> <td><span class="ridge-u"> 0.27</span></td><td>0.29</td> </tr> 
</table>
</td>
</tr>
</table>
</td></tr>

<tr><td>
<table align="center" border="1">
<caption><b>Table:8.5</b> Using Top 250  </caption>
<tr><td>
<table align="center" border="1">
<caption>Baseline methods </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Rerank</td> <td>1.00</td><td>0.74</td><td>1.12</td> <td><span class="dotted-u"> 0.81</span></td><td>0.82</td> </tr>  
<tr><td> DPP</td>    <td>1.00</td><td>0.65</td><td>0.92</td> <td><u>0.71</u></td><td>0.73</td> </tr>  
</table>
</td>
<td>
<table align="center" border="1">
<caption> LSH methods  </caption>
<tr>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr>    <td>0.94</td><td>0.81</td><td>0.95</td> <td><span class="dashed-u"> 0.69</span></td><td>0.73</td> </tr> 
<tr>     <td>0.19</td><td>0.30</td><td>1.30</td> <td><span class="ridge-u"> 0.28</span></td><td>0.30</td> </tr> 
</table>
</td>
</tr>
</table>
</td><td>
<table align="center" border="1">
<caption><b>Table:8.6</b> Using Top 300  </caption>
<tr><td>
<table align="center" border="1">
<caption>Baseline methods </caption>
<tr><td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr> <td>1.00</td><td>0.74</td><td>1.11</td> <td><span class="dotted-u"> 0.80</span></td><td>0.82</td> </tr>  
<tr> <td>1.00</td><td>0.66</td><td>0.94</td> <td><u>0.72</u></td><td>0.73</td> </tr>  
</table>
</td>
<td>
<table align="center" border="1">
<caption> LSH methods  </caption>
<tr>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr>    <td>0.94</td><td>0.81</td><td>0.94</td> <td><span class="dashed-u"> 0.69</span></td><td>0.72</td> </tr> 
<tr>    <td>0.47</td><td>0.62</td><td>1.16</td> <td><span class="ridge-u"> 0.54</span></td><td>0.57</td> </tr> 
</table>
</td>
</tr>
</table>
</td></tr>
</table>
<p align="center">---------------------------- <b> Update: 27/12/2013 - 10/01/2014</b>  ----------------------------</p>
We report the results on multiple runs for various (random)queries in terms of precision(P), sub-topic recall(S), average entropy(AE) and $h_{(P,AE)}$. In these experiments, we split the data as follows

<ul>
<li>Training Data: 200 images per sub-topic totalling to  7K images. </li>
<li>We randomly select 50 images per sub-topic to build the queries and the rest 150 images for validation set(used to fix the hyperparameters). </li>
<li>Testing Data:  1000 images per sub-topic toalling to 35K images. </li>
</ul>

<table align="center" border="1">
<caption>P, S, AE and $h$ of 4 Classes on 50 randomized runs  </caption>
<tr><td>
<table align="center" border="1">
<caption><b>Table 5.1</b>: Baseline methods </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td>
<tr><td> Naive</td>      	 <td>1.00</td><td>0.50</td><td><strike>0.40</strike><br>0.65</td> <td>0.55</td></tr>
<tr><td> Rerank</td>            <td>1.00</td><td>0.74</td><td>1.12</td> <td>0.81</td></tr>
<tr><td> Greedy</td>            <td>0.99</td><td>0.78</td><td>1.15</td> <td>0.82</td></tr>
<tr><td> MMR</td>               <td>0.91</td><td>0.69</td><td>0.85</td> <td>0.63</td></tr>
</table>
</td>
<td>
<table align="center" border="1">
<caption><b>Table 5.2</b>: LSH methods  </caption>
<tr><td> Methods</td>		<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td>
<tr><td> Naive</td>      	 <td>0.98</td><td>0.81</td><td><strike>0.68</strike><br>1.09</td> <td>0.78</td></tr>
<tr><td> Rerank</td>            <td>0.94</td><td>0.81</td><td>0.95</td> <td>0.69</td></tr>
<tr><td> Greedy</td>            <td>0.85</td><td>0.78</td><td>0.91</td> <td>0.62</td></tr>
<tr><td> MMR</td>               <td>0.94</td><td>0.79</td><td>1.06</td> <td>0.75</td></tr>
</table>
</td>
</tr>
</table>
<br>
<table align="center" border="1">
<caption>P, S, AE and $h$ of 4 Classes on 50 randomized runs  </caption>
<tr><td>
<table align="center" border="1">
<caption><b>Table 6.1</b>: Baseline methods </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>1.00</td><td>0.50</td><td>0.65</td> <td>0.55</td><td>0.58</td> </tr>
<tr><td> Rerank</td>            <td>1.00</td><td>0.74</td><td>1.12</td> <td>0.81</td><td>0.82</td> </tr>
<tr><td> Greedy</td>            <td>0.99</td><td>0.78</td><td>1.15</td> <td>0.82</td><td>0.83</td> </tr>
<tr><td> MMR</td>               <td>0.91</td><td>0.69</td><td>0.85</td> <td>0.63</td><td>0.67</td> </tr>
</table>
</td>
<td>
<table align="center" border="1">
<caption><b>Table 6.2</b>: LSH methods  </caption>
<tr><td> Methods</td>	<td>P</td><td>S</td><td>AE</td><td>$h_{avg}$</td><td>$h_{(P,(AE/Z))}$</td>
<tr><td> Naive</td>      	 <td>0.98</td><td>0.81</td><td>1.09</td> <td>0.78</td><td>0.80</td> </tr>
<tr><td> Rerank</td>            <td>0.94</td><td>0.81</td><td>0.95</td> <td>0.69</td><td>0.73</td> </tr>
<tr><td> Greedy</td>            <td>0.85</td><td>0.78</td><td>0.91</td> <td>0.62</td><td>0.68</td> </tr>
<tr><td> MMR</td>               <td>0.94</td><td>0.79</td><td>1.06</td> <td>0.75</td><td>0.78</td> </tr>
</table>
</td>
</tr>
</table>
<ul>
    <li> Naive and MMR perform low for baselines and high for LSH methods. However, Rerank and Greedy show opposite trend(see $h_{(P,AE)}$ values in Table 5).</li>
    <li> For results shown in the last update, we have used 200 images per sub-topic to generate the queries</li>
    <li> Due to space constraints, currently we have only used 50 randomly selected images per sub-topic. </li>
    <li> However, we should be able to randomly select images through out the dataset, which may give uniform trend for all methods.</li>
</ul>  
<b>Similarity Preserving Hash functions</b>: Existence of similarity preserving hash functions are studied in [1] and proved some necessary conditions on similairy measures $sim(x,y) \in [0,1]$ for existence of locality sensitive hash functions satisfying<br> 
			<p align="center"> $Pr_{h \in F}[h(x) = h(y)] = sim(x,y)$</p>
<ul> 
<li><b>Lemma 1</b>: For any similarity function $sim(x,y)$ that admits a locality sensitive hash function families, the distance function $1-sim(x,y)$ satisfies triangle inequality.</li>
<li><b>Lemma 2</b>: Given a locality sensitive hash function family $F$ corresponding to to a similarity function $sim(x,y)$, we can obtain a locality sensitive hash function family $F^{'}$ that maps objects to $\{0,1\}$ and corresponds to the similality function $\frac{1+sim(x,y)}{2}$.</li>
<li><b>Lemma 3</b>: For any similarity function $sim(x,y)$ that admits a locality sensitive hash function families, the distance function $1-sim(x,y)$ is isometrically embeddable in Hamming cube.</li>
<li> It is also shown that procedures used for rounding fractional solutions from linear programs and vector solutions to semi-definite programs can be used to derive similarity preserving hash functions for interesting class of similarity functions.</li>
</ul>
 
<b>LSH with Diversity </b>: Is it possible to design hash functions that improve diversity along with similarity to the given query?
<ul>
	<li> For Greedy we have, $sim(w_{q}, x; x_{r_{1}}, \ldots x_{r_{i-1}}) = (\gamma w_{q}^{T} - \frac{(1-\gamma)}{(i-1)}\Sigma_{j=1}^{i-1} x_{r_{j}}^{T})x$ </li> 
	<li> For MMR we have, $sim(w_{q}, x; x_{r_{1}}, \ldots x_{r_{i-1}}) = \operatorname{arg\,max}_{j \leq (i-1)}(\gamma w_{q}^{T}- (1-\gamma) x_{r_{j}}^{T})x$ </li> 
</ul>
<b>References</b>
<ol>
<li> Moses S Charikar, Similarity estimation techniques from rounding algorithms, STOC, 2002<a href="./fastsvm_files/charikar02.pdf">[pdf]</a></li>
</ol>

<p align="center">---------------------------- <b> Update: 26/12/2013 </b>  ----------------------------</p>
We tried baselines and lsh based methods on a new dataset with the following categories<br>

<ul>
<li>Animal - camel,  cat,  cow,  dog,  sheep  </li>
<li>Bottle - ampule,  beer-bottle,  soda-bottle,  water-bottle,  wine-bottle </li>
<li>Flower - carota,  lily,  rose,  sunflower,  wildpea</li>
<li>Furniture - bed,  bench,  chair,  table,  wallunit</li>
<li>Geography - cave,  iceberg,  mountain,  oceanfloor,  volcano</li>
<li>Music - keyboard,  percussion,  pipe,  violin,  whistle </li>
<li>Vehicle - aeroplane,  bicycle,  car,  roadroller,  train </li>
</ul>
<br>In our experiments, hyperparameters are tuned using the training data (classifiers are constructed  from the same training data) and the best ones are used on the testing data. We use the harmonic mean of precision and entropy, ($h_{(P,AE)}$ ), to enable the results to be compared on one single measure. <br><br>
Training Data: 200 images per sub-topic totalling to 7K images.<br>
Testing Data:  1000 images per sub-topic toalling to 35K images. <br><br>
<u>Observations drawn from our results:</u>

<ul>
    <li> Bottle, Flower, Furniture and Vehicle (4 out of 7) classes show low diversity for naive retrieval and high diversity for Rerank, Greedy and MMR methods.  
    <li> Naive LSH method performs better than naive retrieval. And MMR with lsh performs better than baseline MMR method.</li>
    <li> Harmonic mean of the precision and sub-topic entropy calculated on these 4 classes are shown below.<br></li>
	<table align="center" border="1">
	<caption>Qualitative results for baselines <a href="./viewResults/Nov18_baselineResults.html">here</a> and lsh <a href="./viewResults/Nov18_lshResults.html">here</a></caption>
<tr><td><b>Methods</b></td><td>Naive</td><td>Rerank</td><td>Greedy</td><td>MMR</td> <td>Naive+LSH</td> <td>Rerank+LSH</td><td>Greedy+LSH</td><td>MMR + LSH</td></tr>
<tr><td>$h_{(P,E)}$</td><td> 0.31</td><td> 0.85</td><td>0.88</td><td> 0.75</td>  <td> 0.87 </td> <td> 0.91 </td><td>0.74</td><td>0.90</td></tr>
	</table>
   </li>
    <li>In general, approximate/hash-based similarity search performance(say precision) is lower than the exact/naive similarity search. However, in our case we have both similarity(precision) and dissimilariy(entropy) measures for sub-topic retrieval. In such scenarios, hash-based methods tend to give more diversified results on an average compared to exact similarity based methods.  </li>
</ul>
<br>


<p align="center">---------------------------- <b> Update: 11/11/2013 </b>  ----------------------------</p>

<b>Formulation 3</b>: Given a database of images $X = [x_{1}, \ldots , x_{n}]; \forall i, x_{i} \in R^{d}$ and a hyperplane $w \in R^{d}$, our objective of retrieving top-k diverse images with respect to $w$ is to 
<p align="center">	Maximize $ - \Sigma_{ij}\alpha_{i}\alpha_{j}x_{i}^{T}x_{j}$ </p>
<p align="center">      subject to $ \forall$ $i$, $ \alpha_{i}w^{T}x_{i} \ge 0 $ ; $ \Sigma_{i=1}^{n}\alpha_{i} = k;$ and $\alpha_{i} \in \{0,1\}$ </p><br>

Let $\alpha = [\alpha_{1}, \ldots \alpha_{n}]; u = [w^{T}x_{1}, \ldots, w^{T}x_{n}]$, $A = diag(u)$, $P$ be gram matrix with $P_{ij} = x_{i}^{T}x_{j}$ and $D$ be the diagonal matrix whose entries are $[P_{11},\ldots ,P_{nn}]$, then the above objective is equivalent to
	
<p align="center">	Maximize $ - \frac{1}{2}\alpha^{T}(P-D)\alpha$ </p>
<p align="center">      subject to  $ A\alpha \ge 0$;  $ 1^{T}\alpha = k;$ and $\alpha \in Z^{n}$ </p>

With $Q = \frac{P-D}{2}$, we have

<p align="center">	Minimize $ \alpha^{T}Q\alpha $ </p>
<p align="center">      subject to  $A\alpha \ge 0 $ ; $1^{T}\alpha = k;$ and $\alpha \in Z^{n}$ </p>

The above equation takes the form of a standard Quadratic programming problem with linear constraints. 

<p align="center">---------------------------- <b> Update: 28/9/2013 - 6/10/2013</b>  ----------------------------</p>
<b>Dataset</b>: A subset of imagenet <a href="http://www.image-net.org/">[link]</a> database is chosen. The collected dataset includes 10 classes and each class has 5 subtopics. Each subtopic has 1200 images in the dataset. The classifer are trained on 200 images from each subtopic and obtained 72.78% classification accuracy on the testdata(1000 images per subtopic). <br>
<table align="center" border="1">
<tr><td>
<table align="center">
			<caption><b>Table 4.1</b>: P, S and AE @10  on complete data</caption>
<tr>
	<td>
                <table border="1">
                        <caption>Methods </caption>
                        <tr> <td>Topics</td></tr>
                        <tr> <td>aeroplane</td> </tr>
                        <tr> <td>bench</td></tr>
                        <tr> <td>bicycle</td></tr>
                        <tr> <td>bird</td></tr>
                        <tr> <td>boat</td></tr>
                        <tr> <td>bottle</td></tr>
                        <tr> <td>car</td></tr>
                        <tr> <td>cat</td></tr>
                        <tr> <td>chair</td></tr>
                        <tr> <td>cow</td></tr>
                        <tr><td><b>MEAN</b></td></tr>
                </table>
	</td>
	<td>
		<table align="center" border="1"> <caption>Rerank($\alpha$=0.6)</caption>
			<tr> <td>P </td><td>S</td><td>AE</td></tr>
			<tr><td> 1.00 </td><td> 0.80 </td><td> 1.17 </td></tr>
			<tr><td> 1.00 </td><td> 0.40 </td><td> 0.69 </td></tr>
			<tr><td> 1.00 </td><td> 0.40 </td><td> 0.50 </td></tr>
			<tr><td> 0.90 </td><td> 0.80 </td><td> 0.33 </td></tr>
			<tr><td> 1.00 </td><td> 0.80 </td><td> 1.33 </td></tr>
			<tr><td> 1.00 </td><td> 0.60 </td><td> 1.03 </td></tr>
			<tr><td> 1.00 </td><td> 0.60 </td><td> 0.90 </td></tr>
			<tr><td> 0.90 </td><td> 1.00 </td><td> 0.33 </td></tr>
			<tr><td> 0.90 </td><td> 1.00 </td><td> 0.33 </td></tr>
			<tr><td> 1.00 </td><td> 0.60 </td><td> 0.90 </td></tr>
			<tr><td> 0.97 </td><td> 0.70 </td><td> 0.76 </td></tr>
		</table>
	</td>
	<td>
		<table align="center" border="1"> <caption>Greedy($\alpha$=0.7)</caption>
			<tr> <td>P </td><td>S</td><td>AE</td></tr>
			<tr><td> 1.00 </td><td> 0.60 </td><td> 0.90 </td></tr>
			<tr><td> 1.00 </td><td> 0.60 </td><td> 0.94 </td></tr>
			<tr><td> 1.00 </td><td> 0.40 </td><td> 0.61 </td></tr>
			<tr><td> 1.00 </td><td> 1.00 </td><td> 1.36 </td></tr>
			<tr><td> 1.00 </td><td> 1.00 </td><td> 1.47 </td></tr>
			<tr><td> 1.00 </td><td> 0.60 </td><td> 0.80 </td></tr>
			<tr><td> 1.00 </td><td> 0.60 </td><td> 0.90 </td></tr>
			<tr><td> 1.00 </td><td> 0.80 </td><td> 1.22 </td></tr>
			<tr><td> 0.90 </td><td> 0.80 </td><td> 0.33 </td></tr>
			<tr><td> 1.00 </td><td> 0.60 </td><td> 0.90 </td></tr>
			<tr><td> 0.99 </td><td> 0.70 </td><td> 0.94 </td></tr>
		</table>
	</td>
	<td>
		<table align="center" border="1"> <caption>MMR($\alpha$=0.7)</caption>
			<tr> <td>P </td><td>S</td><td>AE</td></tr>
			<tr><td> 1.00 </td><td> 0.60 </td><td> 0.80 </td></tr>
			<tr><td> 1.00 </td><td> 0.40 </td><td> 0.67 </td></tr>
			<tr><td> 1.00 </td><td> 0.40 </td><td> 0.50 </td></tr>
			<tr><td> 1.00 </td><td> 0.80 </td><td> 1.09 </td></tr>
			<tr><td> 1.00 </td><td> 1.00 </td><td> 1.42 </td></tr>
			<tr><td> 1.00 </td><td> 0.80 </td><td> 1.28 </td></tr>
			<tr><td> 1.00 </td><td> 0.80 </td><td> 1.17 </td></tr>
			<tr><td> 1.00 </td><td> 1.00 </td><td> 1.36 </td></tr>
			<tr><td> 1.00 </td><td> 1.00 </td><td> 1.47 </td></tr>
			<tr><td> 1.00 </td><td> 0.60 </td><td> 0.80 </td></tr>
			<tr><td> 1.00 </td><td> 0.74 </td><td> 1.06 </td></tr>
		</table>
	</td>
	<td>
		<table align="center" border="1"> <caption>No diversity</caption>
			<tr> <td>P </td><td>S</td><td>AE</td></tr>
			<tr><td> 1.00 </td><td> 0.40 </td><td> 0.50 </td></tr>
			<tr><td> 1.00 </td><td> 0.60 </td><td> 0.94 </td></tr>
			<tr><td> 1.00 </td><td> 0.60 </td><td> 1.05 </td></tr>
			<tr><td> 1.00 </td><td> 0.80 </td><td> 1.22 </td></tr>
			<tr><td> 1.00 </td><td> 0.80 </td><td> 1.30 </td></tr>
			<tr><td> 1.00 </td><td> 0.40 </td><td> 0.32 </td></tr>
			<tr><td> 1.00 </td><td> 0.80 </td><td> 1.22 </td></tr>
			<tr><td> 1.00 </td><td> 1.00 </td><td> 1.42 </td></tr>
			<tr><td> 1.00 </td><td> 0.80 </td><td> 1.17 </td></tr>
			<tr><td> 1.00 </td><td> 0.60 </td><td> 0.94 </td></tr>
			<tr><td> 1.00 </td><td> 0.68 </td><td> 1.01 </td></tr>
		</table>
	</td>
</tr>
</table>
</td>
</tr>
</table>

<table align="center" border="1">
<caption><b>Table 5:Performance with LSH on ImageNet database</b> </caption>
	<tr><td>Methods</td><td> P </td><td> SR</td><td> AE</td><td>$h_{P,AE}$</td></tr>
	<tr><td> No diversity</td>      <td>0.99</td><td>0.80</td><td>0.70</td> <td>0.79</td></tr>
	<tr><td> Rerank</td>            <td>0.96</td><td>0.90</td><td>0.80</td> <td>0.87</td></tr>
	<tr><td> Greedy</td>            <td>0.94</td><td>0.82</td><td>0.71</td> <td>0.80</td></tr>
	<tr><td> MMR</td>               <td>0.97</td><td>0.90</td><td>0.84</td> <td>0.90</td></tr>
</table>
<ul>
</ul>
<b>Issues</b>
<ul>
	<li> How to characterize diversity for real datasets when each class has varying degree of diversity i.e, number of subtopics in each class.</li>
	<li> Even when classes have same number of subtopics, each class might require different degree of diversity (evident from our initial experiments). Instead of averaging, we may have to analayse each class separately</li>

</ul>

<p align="center">---------------------------- <b> Update: 27/9/2013</b>  ----------------------------</p>

<b>Revisiting the problem of image diversity</b>: Given a database of images $X = [x_{1}, \ldots , x_{n}]; \forall i, x_{i} \in R^{d}$ and a hyperplane $w \in R^{d}$, our objective of retrieving top-k diversified images with respect to $w$ is to 
<p align="center">	Maximize $ \Sigma_{i=1}^{n}\alpha_{i}w^{T}x_{i} - \Sigma_{ij}\alpha_{i}\alpha_{j}x_{i}^{T}x_{j}$ </p>
<p align="center">      subject to $ \Sigma_{i=1}^{n}\alpha_{i} = k;$ and $\alpha_{i} \in \{0,1\}$ </p><br>

Let $\alpha = [\alpha_{1}, \ldots \alpha_{n}]; u = [w^{T}x_{1}, \ldots, w^{T}x_{n}]$, $P$ be gram matrix with $P_{ij} = x_{i}^{T}x_{j}$ and $D$ be the diagonal matrix whose entries are $[P_{11},\ldots ,P_{nn}]$, then the above objective is equivalent to
	
<p align="center">	Maximize $ u^{T}\alpha - \frac{1}{2}\alpha^{T}(P-D)\alpha$ </p>
<p align="center">      subject to $\alpha^{T}1 = k;$ and $\alpha \in Z^{n}$ </p>

With simple substitutions $c = -u$ and $Q = \frac{P-D}{2}$, we have

<p align="center">	Minimize $ \alpha^{T}Q\alpha + c^{T}\alpha $</p>
<p align="center">      subject to $\alpha^{T}1 = k;$ and $\alpha \in Z^{n}$ </p>

Quadratic programming problems of this form are solved using branch-and-bound techniques<sup>[1][2]</sup>. 

<br>
<b>References</b>
<ol>
	<li><i>Buchheim, Christoph and Caprara, Alberto and Lodi, Andrea</i> An Effective Branch-and-Bound Algorithm for Convex Quadratic Integer Programming, Mathematical Programming, Springer, 2010 <a href="http://www.optimization-online.org/DB_FILE/2010/09/2731.pdf">[pdf]</a>
	<li><i>Buchheim, Christoph and De Santis, Marianna and Palagi, Laura and Piacentini, Mauro</i> An Exact Algorithm for Nonconvex Quadratic Integer Minimization using Ellipsoidal Relaxations, 2012 <a href="http://www.optimization-online.org/DB_FILE/2012/05/3476.pdf">[pdf]</a>
</ol>

<p align="center">---------------------------- <b> Update: 21/9/2013 - 26/9/2013</b>  ----------------------------</p>
<b>Dataset</b>: 7 classes with each class having 5 subtopics. 85 images for each subtopic are obtained from Caltech-256<sup>[1]</sup> dataset totalling to 2975 images.<br>
<ul>
<li>animals - camel, elephant, gorilla, horse, leopards </li>
<li>architect - light-house,  minaret, radio-telescope, skyscraper, tower-pisa</li>
<li>birds - cormorant, duck, hummingbird, ibis, owl</li>
<li>computers - monitor, mouse, head-phones, pci-card, video-projector</li>
<li>insects - butterfly, centipede, cockroach, grasshopper, spider</li>
<li>plants - bonsai, cactus, fern, iris, palm-tree</li>
<li>scenes - comet, fireworks, lightning, rainbow, waterfall </li>
</ul>
<b>Experiment Setting</b>: Each image is represented using SIFT features with 800 visual words as implemented in VLFeat Library.  60 images per subtopic are used for training and the rest for testing.  Liblinear is used to obtain classifers for each of the 7 class with 74.86% accuracy on the test data.<br>
<b>Evaluation metrics </b>: 
<ul>
	<li>Precision(P): Fraction of  relevant images retrieved among all the retrieved images.</li>
	<li>S-recall(S)<sup>[3]</sup>: Fraction of sub-topics retrieved over all sub-topics for the given topic. </li>
	<li>Average Entropy(AE): $-\Sigma_{i=1}^{n} f_{i}\log f_{i}$ , where $n$ is the number of sub-topics for the given topic, $f_{i}$ is the fraction of images of $i^{th}$ sub-topic in the retrieved images. </li>
</ul>
<b>Results</b>:
<table align="center" border="1">
<tr><td>
	<table align="center" border="1">
		<caption><b>Table 2.1</b>: P, S and AE @10 on complete test data</caption>
		<tr><td>Methods</td><td>$\alpha$</td><td> P </td><td> SR</td><td> AE</td></tr>
		<tr><td> No diversity</td><td>N/A</td> <td> 0.96 </td> <td> 0.63 </td><td> 0.82 </td></tr>
		<tr><td> Rerank</td><td>0.70</td> <td> 0.97 </td> <td> 0.69 </td><td> 0.73 </td></tr>
		<tr><td> Greedy</td><td>0.60</td> <td> 0.91 </td> <td> 0.71 </td><td> 0.86 </td></tr>
		<tr><td> MMR</td><td>0.70</td> <td> 0.93 </td> <td> 0.69 </td><td> 0.96 </td></tr>
	</table>
</td>
<td>
	<table align="center" border="1">
		<caption><b>Table 2.2</b>: 10 images randomly sampled from testdata </caption>
		<tr><td>Methods</td><td>$\alpha$</td><td> P </td><td> SR</td><td> AE</td></tr>
		<tr><td> No diversity</td><td>N/A</td> <td> 0.95 </td> <td> 0.76 </td><td> 0.91 </td></tr>
		<tr><td> Rerank</td><td>0.70</td> <td> 0.93 </td> <td> 0.83 </td><td> 1.06 </td></tr>
		<tr><td> Greedy</td><td>0.70</td> <td> 0.92 </td> <td> 0.76 </td><td> 0.93 </td></tr>
		<tr><td> MMR</td><td>0.70</td> <td> 0.89 </td> <td> 0.74 </td><td> 0.95 </td></tr>
	</table>
</td>
<td>
	<table align="center" border="1">
		<caption><b>Table 2.3</b>: 15 images randomly sampled from testdata </caption>
		<tr><td>Methods</td><td>$\alpha$</td><td> P </td><td> SR</td><td> AE</td></tr>
		<tr><td> No diversity</td><td>N/A</td> <td> 0.96 </td> <td> 0.69 </td><td> 0.84 </td></tr>
		<tr><td> Rerank</td><td>0.70</td> <td> 0.90 </td> <td> 0.74 </td><td> 0.86 </td></tr>
		<tr><td> Greedy</td><td>0.70</td> <td> 0.93 </td> <td> 0.71 </td><td> 0.86 </td></tr>
		<tr><td> MMR</td><td>0.70</td> <td> 0.91 </td> <td> 0.72 </td><td> 0.91 </td></tr>
	</table>
</td>
</tr>
</table>
<ul> 
Notice that there is a drop in precision values while increase in the S-recall and average entropy values which is a measure of diversity. ($AE_{opt}$ is 1.6)
<li><b>Table 2.1</b> shows results obtained on the test data. </li>
<li><b>Table 2.2</b> shows results averaged over 50 random samplings of 10 images from each subtopic. </li>
<li><b>Table 2.3</b> shows results averaged over 50 random samplings of 15 images from each subtopic. </li>
</ul>

<b>Experiments on another small Dataset</b>: We use the dataset in k-ddp<sup>[5]</sup> which has 3 classes - cars(8 subtopics), cities(12 subtopics), dogs(12 subtopics). 39 images per subtopic are used to train the classifiers and 20 images for testing. Accuracy on test data is 98.75%. ($AE_{opt}$ is 2.2 )
<table border="1" align="center">
<tr>
<td>
	<table align="center" border="1">
		<caption><b>Table 3.1</b>: P, S and AE @10 </caption>
		<tr><td>Methods</td><td>$\alpha$</td><td> P </td><td> SR</td><td> AE</td></tr>
		<tr><td> No diversity</td><td>N/A</td> <td> 1 </td> <td> 0.61 </td><td> 1.76 </td></tr>
		<tr><td> Rerank</td><td>0.60</td> <td> 1 </td> <td> 0.71 </td><td> 1.93 </td></tr>
		<tr><td> Greedy</td><td>0.60</td> <td> 1 </td> <td> 0.68 </td><td> 1.87 </td></tr>
		<tr><td> MMR</td><td>0.60</td> <td> 1 </td> <td> 0.68 </td><td> 1.85 </td></tr>
	</table>
</td>
<td>
	<table align="center" border="1">
		<caption><b>Table 3.2</b>: P, S and AE @20</caption>
		<tr><td>Methods</td><td>$\alpha$</td><td> P </td><td> SR</td><td> AE</td></tr>
		<tr><td> No diversity</td><td>N/A</td> <td> 1 </td> <td> 0.86 </td><td> 2.08 </td></tr>
		<tr><td> Rerank</td><td>0.60</td> <td> 1 </td> <td> 0.89 </td><td> 2.05 </td></tr>
		<tr><td> Greedy</td><td>0.60</td> <td> 1 </td> <td> 0.89 </td><td> 2.05 </td></tr>
		<tr><td> MMR</td><td>0.60</td> <td> 1 </td> <td> 0.89 </td><td> 2.05 </td></tr>
	</table>
</td>
</tr>
</table>
<b>References</b>
<ol>
	<li><i>Griffin, G. Holub, AD. Perona, P.</i> The Caltech 256. Caltech Technical Report, 2007 <a href="http://www.vision.caltech.edu/Image_Datasets/Caltech256/">[link]</a></li>
	<li><i>R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin.</i> LIBLINEAR: A library for large linear classification Journal of Machine Learning Research, 2008 <a href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">[link]</a> </li>
	<li><i>Cheng Zhai, William W. Cohen, John Lafferty</i>: Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval, SIGIR 03 <a href="http://www.cs.cmu.edu/~wcohen/postscript/sigir-2003.pdf">[pdf]</a></li>
	<li><i>Jaime Carbonell , Jade Goldstein</i>: The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries, SIGIR, 1998 <a href="http://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf">[pdf]</a></li>
	<li><i>Alex Kulesza, Ben Taskar</i> :  k-DPPs: Fixed-Size Determinantal Point Processes, ICML 11 <a href="http://www.seas.upenn.edu/~taskar/pubs/kdpps_icml11.pdf">[pdf]</a> </li>
</ol> 

<p align="center">---------------------------- <b> Update: 13/9/2013 - 20/9/2013</b>  ----------------------------</p>
Implementation of k-DPP paper: A brief description of the experimental setting and mathematical notations used is available here <a href="experiments_dpp.pdf">[pdf]</a><br><br>

<table border="1" align="center">
<caption> Table 1: Percentage of image search examples judged the same way as the majority of human annotators.</caption>
<tr>
		<td><table border="1" align="center">
			<caption>Vocabulary size </caption>
			<tr><td><b>Objective</b></td><td> <b>Methods</b></td></tr> 
			<tr><td>$\text{argmin}_{i \in C_{t}} [\Sigma_{j \in Y_{t}} x_{i}^{T}x_{j}]$</td><td> Mean diversity</td></tr> 
			<!--<tr><td>$ \text{argmax}_{i \in C_{t}} [\alpha w^{T}x_{i} - (1 - \alpha)\frac{\Sigma_{j \in Y_{t}} x_{i}^{T}x_{j}}{|Y_{t}|}]$</td><td> SVM relevance + mean diversity + $\alpha$=0.2</td> </tr>-->
			<tr><td>$ \text{argmax}_{i \in C_{t}} [\alpha w^{T}x_{i} -(1-\alpha) max_{j \in Y_{t}} x_{i}^{T}x_{j}]$</td><td> MMR<sup>[2]</sup> + $\alpha$=0.2 </td> </tr>
			<tr><td>$\text{argmin}_{i \in C_{t}} [max_{j \in Y_{t}} x_{i}^{T}x_{j}]$</td><td> MMR + $\alpha$=0</td> </tr>
			<tr><td>$\text{argmax}_{i \in C_{t}} [P^{k}(Y_{t} \cup \{i\})]$</td><td> k-DPP<sup>[1]</sup> </td></tr>
		</table></td>
		<td><table border="1" align="center">
			<caption> 800 words </caption>
			<tr align="center"><td> <b>cars</b></td> <td><b>cities</b></td><td><b>dogs</b></td><td>Average</td></tr>
			<tr align="center"><td>51.45</td> <td>52.70</td> <td>50.77</td> <td> 51.64</td> </tr>
			<!--<tr align="center"><td>50.62</td> <td>50.74</td> <td>50.11</td> <td> 50.49</td> </tr>-->
			<tr align="center"><td>54.56</td> <td>47.79</td> <td>50.99</td> <td> <span class="dotted-u">51.11</span></td> </tr>
			<tr align="center"><td>56.02</td> <td>49.75</td> <td>50.11</td> <td> <u>51.96</u></td> </tr>
			<tr align="center"><td>49.38</td> <td>55.64</td> <td>48.57</td> <td> <b>51.20</b></td> </tr>
		</table></td>
		<td><table border="1" align="center">
			<caption> 2400 words </caption>
			<tr align="center"><td> <b>cars</b></td> <td><b>cities</b></td><td><b>dogs</b></td><td>Average</td></tr>
			<tr align="center"><td>51.45</td> <td>51.96</td> <td>50.99</td> <td> 51.46</td></tr>
			<!--<tr align="center"><td>49.79</td> <td>52.21</td> <td>50.33</td> <td> 50.77</td></tr>-->
			<tr align="center"><td>55.19</td> <td>50.74</td> <td>49.89</td> <td> <span class="dotted-u">51.94</span></td></tr>
			<tr align="center"><td>56.64</td> <td>50.98</td> <td>50.99</td> <td> <u>52.87</u></td></tr>
			<tr align="center"><td>49.79</td> <td>55.39</td> <td>55.38</td> <td> <b>53.52</b></td></tr>
		</table></td>
</tr>
</table>
In Table 1, 
<ul>
	<li>SIFT features with kchi2 kernel were used for all the methods.</li>
	<li>$\alpha$ is a parameter controlling the relative importance of relevance and diversity.</li>
	<li>Accuracy is computed on the complete dataset used in the k-DPP<sup>[1]</sup> </li>
</ul>
Few Issues: 
<ul>
	<li>To evaluate on other datasets, we need to have human annotators for this experiment!</li>
	<li>How to evaluate the efficiency of diverse image retrieval in this setting? </li>
</ul>
<b>References</b>
<ol>
<li><i>Alex Kulesza, Ben Taskar</i> :  k-DPPs: Fixed-Size Determinantal Point Processes, ICML 11 <a href="http://www.seas.upenn.edu/~taskar/pubs/kdpps_icml11.pdf">[pdf]</a> </li>
<li><i>Jaime Carbonell , Jade Goldstein</i>: The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries, SIGIR, 1998 <a href="http://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf">[pdf]</a></li>
</ol>
<p align="center">---------------------------- <b> Update: 06/09/2013 </b>  ----------------------------</p>
[contd..] <b> Quantifying image diversity</b> <br>
For subtopic retrieval task using MMR<sup>[3]</sup>, we quantify diversity in terms of sub-topic recall and precision. If $S$ is some IR system that produces rankings and $r$ is a recall level, $0 \leq r \leq 1$, define the sub-topic recall and precision as <br>
<ul>
	<li> <i>S-recall</i> at $ K = \frac{|\cup_{i=1}^{K} subtopics(s_{i})|}{n_{ST}}$, where $n_{ST}$ is the number of subtopics for the given topic</li>
	<li><i>S-precision</i> at <i>r</i> $ = \frac{minRank(S_{opt}, r)}{minRank(S, r)}$, where minRank($S$, $r$) is the minimal rank $K$ at which the ranking produced by $S$ has $S$-recall $r$</li>
</ul>
 
<b>Dataset</b>: image dataset from DPP<sup>[1]</sup>  <br>
<table border="1" align="center">
			<caption>S-recall @10 and @20 for different methods</caption>
<tr>
	<td>
		<table border="1">
			<caption>No diversity</caption>
			<tr> <td>Topic</td><td> @10 </td><td> @20</td> </tr>
			<tr> <td>cars</td> <td> 0.88 </td><td> 1.00 </td></tr>
			<tr> <td>cities</td><td> 0.50 </td><td> 0.75 </td></tr>
			<tr> <td>dogs</td><td> 0.50 </td><td> 0.83 </td></tr>
		</table>
	</td>
	<td>
		<table border="1">
			<caption>Greedy ranking</caption>
			<tr> <td>Topic</td><td> @10 </td><td> @20</td> </tr>
			<tr> <td>cars</td><td> 0.50 </td><td> 1.00 </td></tr>
			<tr> <td>cities</td><td> 0.50 </td><td> 0.67 </td></tr>
			<tr> <td>dogs</td> <td> 0.67 </td><td> 0.75 </td></tr>
		</table>
	</td>

	<td>
		<table border="1">
			<caption>Reranking </caption>
			<tr> <td>Topic</td><td> @10 </td><td> @20</td> </tr>
			<tr> <td>cars</td><td> 0.88 </td><td> 0.88 </td> </tr>
			<tr> <td>cities</td> <td> 0.67 </td><td> 0.83 </td></tr>
			<tr> <td>dogs</td><td> 0.58 </td><td> 0.67 </td></tr>
		</table>
	</td>
	<td>
		<table border="1">
			<caption>MMR retireval</caption>
			<tr> <td>Topic</td><td> @10 </td><td> @20</td> </tr>
			<tr> <td>cars</td> <td> 0.88 </td><td> 0.88 </td></tr>
			<tr> <td>cities</td><td> 0.33 </td><td> 0.75 </td></tr>
			<tr> <td>dogs</td><td> 0.50 </td><td> 0.67 </td></tr>
		</table>
	</td>
</tr>
</table>
 Results demonstrate that retieval without diversification does infact have high sub-topic recall!!<br>
<b>References</b>
<ol>
<li><i>Alex Kulesza, Ben Taskar</i> : Determinantal Point Processes for machine learning, arxiv, 2012 <a href="http://arxiv.org/abs/1207.6083">[link]</a> </li>
<li><i>Cheng Zhai, William W. Cohen, John Lafferty</i>: Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval, SIGIR 03 <a href="http://www.cs.cmu.edu/~wcohen/postscript/sigir-2003.pdf">[pdf]</a></li>
<li><i>Jaime Carbonell , Jade Goldstein</i>: The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries, SIGIR, 1998 <a href="http://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf">[pdf]</a></li>
</ol>
<p align="center">---------------------------- <b> Update: 30/08/2013</b>  ----------------------------</p>
 
<b>Quantifying image diversity</b> 
<ul>
	<li> <i>Using lossless JPG file size of average image: </i>
		<ul>
			<li> Compute the average image of retrieved images and measure lossless JPG file size which reflects the amount of information in an image. </li>
			<li> Diverse images will result in a blurrier average image, the extreme being gray image, whereas images with little diversity will result in a more structured, sharper average image. </li>
			<li> Therefore, a smaller JPG file size of average image implies a more diverse set of images.</br> 
		</ul>
	</li>
	<li> <i>Using Entropy as a measure of sub-topic diversity </i>: <font color="red">issues with dataset</font> 
	</li>
	<li> <i>Using Average Precision as a measure of sub-topic diversity</i>: <font color="red">issues with dataset</font>
	</li>
</ul>
<b>Baseline Results</b>
<ul>
	<li><b>ImageNet</b>: Using lossless JPG file size of average image as a measure of diversity, the greedy approach performs reasonably well. (see <a href="imagenet_results.html">here</a>) </li>
	<li><b>ImageCLEF</b>: Most of the existing works use both visual and textual vocabularies as feature representations.(see <a href="imageclef_results.html">here</a>) </li>
</ul>


<p align="center">---------------------------- <b> Update: 23/08/2013</b>  ----------------------------</p>
<b> Experiments </b>:<br>
<table>
<caption> Using liblinear svm solver: PR-curve for ImageNet (left) and ImageCLEF (right).</caption>
<tr>
	<td> <img src= "prcurve-imagenet-liblinear.png" /> </td>
	<td><img src= " prcurve-imagecleff-800-liblinear.png" /> </td>
</tr>
</table>

<p align="center">---------------------------- <b> Update: 16/08/2013</b>  ----------------------------</p>

<b> Datasets</b>: ImageNet (<a href="http://image-net.org/download-features">link</a>); ImageCLEF (<a href="http://www.imageclef.org/datasets">link</a>); MIRFLICKR (<a href="http://press.liacs.nl/mirflickr/">link</a>) <br> 
ImageNet benchmarks: <a href="http://www.image-net.org/papers/imagenet_cvpr09.pdf">cvpr09</a> <a href="http://vision.stanford.edu/documents/DengBergLiFei-Fei_ECCV2010.pdf">eccv10</a> <a href="http://www.iri.upc.edu/download/scidoc/1350">cvpr11</a><br>
ImageClef benchmarks: <a href="http://lear.inrialpes.fr/pubs/2010/MCPSV10/LEAR.XRCE.ImageClef.2010.pdf">conceptdetection2010</a><br>
Mirflickr benchmarks: <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.174.2949&rep=rep1&type=pdf">photoannotation2010</a>

<p align="center">---------------------------- <b> Update: 2/08/2013</b>  ----------------------------</p>

<!-- %% Greedy Formulation -->
<b>Greedy Formulation</b>: Given a partial result of ranked list  $x_{r_{1}}, x_{r_{2}},\ldots ,x_{r_{J}}$, next result $x^{*}$ to be added to the list at position $J+1$ as  
	
<p align="center">	$N(x^{*};(x_{r_{1}}, x_{r_{2}},\ldots ,x_{r_{J}})) = \frac{1}{J}\Sigma_{j=1}^{J}D_s(x^{*}, x_{r_{j}})$  ----- (1)</p>

	where $D_s(x_{i},x_{j}) = -x_{i}^Tx_{j}$ is the dissimilarity measure between the images $x_{i},x_{j}$. However, the retrieved result, $x^{*}$, must be relevant to the given query i.e.,<br>
<p align="center"> $S_s(w, x^{*}) = w^{T}x^{*}$ ----- (2) </p>

Combining (1) and (2) with an appropriate choice of $\alpha$, 
<p align="center">$R(x^{*};w, (x_{r_{1}}, x_{r_{2}},\ldots, x_{r_{J}})) = \alpha S_s(w, x^{*}) + (1-\alpha)&nbsp;N(x^{*};(x_{r_{1}}, x_{r_{2}},\ldots ,x_{r_{J-1}}))$ ----- (3)</p>
	<!--<b>Objective Function</b>:-->
Therefore, the optimal ranking is one that maximizes the following objective over all rankings
<p align="center"> $F(w;(x_{r_{1}}, \ldots, x_{r_{k}})) = \Sigma_{i=1}^{k} R(x_{r_{i}};w, (x_{r_{1}}, x_{r_{2}},\ldots ,x_{r_{i-1}}))$</p>
<p align="center"> $ = \Sigma_{i=1}^{k} [ \alpha w^{T}x_{r_{i}} + \frac{1-\alpha}{i}&nbsp;\Sigma_{j=1}^{i}D_s(x_{r_{i}}, x_{r_{j}})]$  ----- (4)</p>
<p align="center"> i.e., $(\hat{r_{1}}, \hat{r_{2}}, \ldots, \hat{r_{k}}) =  \text{argmax}_{(r_{1},\ldots,r_{k})} F(w;(x_{r_{1}}, \ldots, x_{r_{k}}))$</p>
(<u>Can we quantify the degree of "diversity" or a metric  in the retrieved images?</u>) <br>

<p align="center">---------------------------- <b> Update: 26/07/2013</b>  ----------------------------</p>

<!-- %% LP Formulation -->
<b>MRF Energy Maximization</b>: As many vision problems, this problem can be naturally formulated in terms of energy maximization, where the energy takes the form <br>
<p align="center">	$ \text{argmax}_{(\alpha_{1},\ldots, \alpha_{n})}E(X;w) = \Sigma_{i=1}^{n}\alpha_{i}w^{T}x_{i} - \Sigma_{ij}\alpha_{i}\alpha_{j}x_{i}^{T}x_{j}$ ---- (1a)</p>
<p align="center">      subject to $ \Sigma_{i=1}^{n}\alpha_{i} = k;$ and $\alpha_{i} \in \{0,1\}$ </p><br>
However, maximizing energy in (1a) is not trivial, as the pairwise term is nonsubmodular function.<br>
(<u>Can we truncate the nonsubmodular term i.e., replace with a submodular approximation and maximize (1a) later? </u>)<br>

<p align="center">---------------------------- <b> Update: 19/07/2013</b>  ----------------------------</p>

<b>Goal:</b> Objective is to speed up SVM evaluations with respect to any query(SVM hyperplane). We need to preprocess the image database such that top-$k$ ranked images can be retrieved without performing an exhaustive linear scan.(some related work on SVM indexing is <a href = "relatedwork.html">here</a>)<br><br>
<center>
<img src="fastsvm_files/fastsvm.jpg"></img>
</center>
<br>
<b>Definition</b>: Given a database of images $X = {\{x_{j}\}_{j=1}^{n}}$, and normal to the SVM hyperplane, $w$, representing a visual category, the top-$k$ relevant images of the same class are $(x_{r_{1}}, x_{r_{2}}, \ldots, x_{r_{k}}) \in S$ such that $ \Sigma_{i=1}^{k}w^{T}x_{r_{i}}$ is maximized over all possible choices of $r_{1}, \ldots ,r_{k}$ with $r_{i} \ne r_{j}$ if $i \ne j$. <br><br>
<b>Problem</b>: Need to retrieve top-k diversified images relevant to the given query hyperplane. (some related work on diversification in image retrieval is <a href="http://www.multimediaeval.org/mediaeval2013/diverseimages2013/">here</a>) <br> 


</div>
</body></html>
